{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Analysis \n",
    "## Section 2: Prepare the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "- ~~create dedicated github project repository~~ https://github.com/avidrucker/vocab-analysis\n",
    "- break up notebook into 5 sections/partitions as per \"DataScienceProcessTips.pdf\" **in_progress**\n",
    "- save out csv after each section, label \"section_#_output\", import into the following section\n",
    "    - ~~section 2 notes~~\n",
    "    - ~~section 2 cards~~\n",
    "    - section 2 revlogs\n",
    "    - section 2 note_card_combo\n",
    "        - binary only\n",
    "        - non-binary only\n",
    "        - full combo\n",
    "- consolidate all paper todos, critique advices, etc. to this document\n",
    "- upload this document to the dedicated github project repository\n",
    "- import in Section 1 from personal laptop where question has been identified\n",
    "- import in review (revlog) data (see model info here: https://github.com/ankidroid/Anki-Android/wiki/Database-Structure )\n",
    "- combine revlogs w/ combo dataframe to tabulate review time, review count, lapse count, for both per card and per note\n",
    "- add todos from email reminder (see email inbox)\n",
    "- clearly name card, note, and revlog dataframes as numbered steps **in_progress**\n",
    "    - cards\n",
    "    - ~~notes~~\n",
    "    - revlogs\n",
    "- suffix \"final\" to card, note, and revlog dataframes to clearly denote analysis readiness\n",
    "    - cards\n",
    "    - ~~notes~~\n",
    "    - revlogs\n",
    "- mark cells for export to sections 3, 4 **in_progress**\n",
    "- export section 3 & section 4 cells to next documents\n",
    "- ~~link to presentation~~: https://docs.google.com/presentation/d/1UntQmGL2uhH9POCQzyVEh2j7qviHmrZjjyHJttlvVWU/edit?usp=sharing\n",
    "- create new graphs (use broad survey & quick correlation graphs from recent exercises/modules)\n",
    "- update presentation with new graphs\n",
    "- assign (in section 2) JLPT \"N\" levels to each word w/ a JLPT \"N\" tag\n",
    "- fix date conversions to occur before dataframe merge, not after **in_progress**\n",
    "- re-export Anki collection from PC into project, then unzip & rerun with this notebook\n",
    "- export all utility functions to be used in other notebooks **in_progress**\n",
    "- Remove sentences, questions & phrases to prevent skewing of data & to hone in on vocabulary trends first. The above can be added back, or analyzed separately, after. (remove sentences, questions & phrases, idioms too)\n",
    "- ~~remove words with 0 reps from primary graphing, as they dilute study consequence correlations/readings/analyses~~\n",
    "- Before making graphs & charts, prepare a dataframe with only numerical data (binary, non-binary, and (?) both)\n",
    "    - ie. remove: 'Yomi1', 'Translation','Translation2', 'Translation3', 'AlternateForms', 'PartOfSpeech', 'Sound', 'Sound2', 'Sound3', 'Examples', 'ExamplesAudio', 'AtoQ','AtoQaudio', 'AtoQkana', 'AtoQtranslation', 'QandApicture','answerPicture', 'Meaning1', 'SimilarWords', 'RelatedWords','Breakdown1', 'Comparison', 'Usage', 'Prompt1', 'Prompt2','KakuMCD', 'IuMCD', 'ExtraMemo', 'Yomi2', 'Meaning2', 'Breakdown2','Picture1', 'Picture2', 'Picture3', 'Picture4', 'HinshiMarker','Hint', 'Term2', 'ArabicNumeral', 'CounterKanji', 'Mnemonic','SameSoundWords', 'Yomi3', 'gChap', 'gBook', 'semester', 'gNumber','Transliteration', 'SoloLookCards', 'TagOverflow', 'blank1','blank2',\n",
    "- add 'commonword' tag to tags from 'PartOfSpeech' column\n",
    "- fix ordering (1) tag removal, renaming & confirmation (tag frequency), then (2) dataframe merges, then (3) addition of binary columns\n",
    "\n",
    "for next time:\n",
    "- we want to understand the words conceptually: abstract vs concrete, verbs vs nouns vs adjectives\n",
    "- conduct data entry to add concrete boolean for each note\n",
    "- inspect mod as a marker for 'freshness'\n",
    "- notes: logistic regression: classification/categorization\n",
    "\n",
    "Further Ideas:\n",
    "- generate power as work (reps) over time\n",
    "- use lapses to calculate efficiency per word\n",
    "- generate stress as ratio of lapses to reps, compare wtih ease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import json\n",
    "from datetime import datetime, timedelta, date\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = \"datasets/collection.anki2\"\n",
    "cnx = sqlite3.connect(location) # create sql file connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TDD backbone assertion to confirm a function call returns the desired result\n",
    "def assertEquals(actual, expected, desc):\n",
    "    assert(actual==expected), desc + \" result: \" + str(actual) + \", expected: \" + str(expected)\n",
    "    return \"OK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A word anout how the data was stored\n",
    "\n",
    "Anki, the Open Source, Spaced Repetition software (& app & service), saves a student's data in a few locations. There are the \"Notes\" which are the raw info used to make cards (fields of vocab data, metatags, trivia facts, images, audio, etc..) Then there are the \"Cards\" which are the actual studied items, where the study overview data is stored (such as study date-times, repetitions (reviews), intervals (how long a card is to be remembered), lapses (forgets & subsequent interval resets)). Additionally, data concerning the entire collection is stored under something cryptically called \"Columns\". Lastly, there is a \"RevLog\" which contains all the study data in detail for each individual repetition (study datetime, card studied, etc..) This document was critical to piecing together the puzzle: https://github.com/ankidroid/Anki-Android/wiki/Database-Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extract Deck Creation Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2013-01-08 09:00:00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = pd.read_sql_query(\"SELECT * FROM col\", cnx)\n",
    "crt = df_c['crt'][0] # save collection creation date (in epoch time)\n",
    "pd_crt = pd.to_datetime(crt, unit = 's')\n",
    "print(pd_crt)\n",
    "\n",
    "assertEquals(str(pd_crt), \"2013-01-08 09:00:00\", \"Collection Creation Date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Extract field names to label columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "field_names = []\n",
    "for row_index, blob in df_c['models'].items():\n",
    "    for model_id, data in json.loads(blob).items():\n",
    "        field_names += list(map(lambda fld: fld['name'], data['flds']))\n",
    "field_names.append('Tags')\n",
    "expected_names = ['Term', 'Yomi1', 'Translation', 'Translation2', 'Translation3', 'AlternateForms', 'PartOfSpeech', 'Sound', 'Sound2', 'Sound3', 'Examples', 'ExamplesAudio', 'AtoQ', 'AtoQaudio', 'AtoQkana', 'AtoQtranslation', 'QandApicture', 'answerPicture', 'Meaning1', 'SimilarWords', 'RelatedWords', 'Breakdown1', 'Comparison', 'Usage', 'Prompt1', 'Prompt2', 'KakuMCD', 'IuMCD', 'ExtraMemo', 'Yomi2', 'Meaning2', 'Breakdown2', 'Picture1', 'Picture2', 'Picture3', 'Picture4', 'HinshiMarker', 'Hint', 'Term2', 'ArabicNumeral', 'CounterKanji', 'Mnemonic', 'SameSoundWords', 'Yomi3', 'gChap', 'gBook', 'semester', 'gNumber', 'Transliteration', 'SoloLookCards', 'TagOverflow', 'blank1', 'blank2', 'Tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertEquals(field_names, expected_names, \"Field Names\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Import card study data into data frame \"df_cards\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 387,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 6: Take in study data from Anki collection\n",
    "df_cards = pd.read_sql_query(\"SELECT * FROM cards\", cnx)\n",
    "assertEquals(df_cards.shape[0],19315,\"Rows\")#6386, 21979, 19363\n",
    "assertEquals(df_cards.shape[1],18,\"Columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Confirm that card data model matches expected format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 388,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expected_columns_1 = ['id', 'nid', 'did', 'ord', 'mod', 'usn', 'type', 'queue', 'due', 'ivl', 'factor',\n",
    " 'reps', 'lapses', 'left', 'odue', 'odid', 'flags', 'data']\n",
    "\n",
    "def lists_equal(a,b):\n",
    "    return (a == b).all()\n",
    "\n",
    "assertEquals(lists_equal(df_cards.columns.values, expected_columns_1), True, \"Card Columns Import\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Shallow check for duplicates (matching rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    " def has_dupes(df_in):\n",
    "    dupe = df_in.duplicated()\n",
    "    return df_in.loc[dupe].shape[0] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertEquals(has_dupes(df_cards), False, \"Duplicates Not Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Remove unneeded card dataframe columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_line_break():\n",
    "    print(\"-\"*75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_before_after(b, a, t=\"\"):\n",
    "    if t != \"\":\n",
    "        print_line_break()\n",
    "        print(t)\n",
    "    print_line_break()\n",
    "    print(\"Before: \" + str(b))\n",
    "    print_line_break()\n",
    "    print(\"After: \" + str(a))\n",
    "    print_line_break()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Card Columns:\n",
      "---------------------------------------------------------------------------\n",
      "Before: ['id' 'nid' 'did' 'ord' 'mod' 'usn' 'type' 'queue' 'due' 'ivl' 'factor'\n",
      " 'reps' 'lapses' 'left' 'odue' 'odid' 'flags' 'data']\n",
      "---------------------------------------------------------------------------\n",
      "After: ['id' 'nid' 'ord' 'queue' 'due' 'ivl' 'factor' 'reps' 'lapses']\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards_001_less_cols = df_cards.copy()\n",
    "df_cards_001_less_cols = df_cards_001_less_cols.drop(['did','usn','type','mod','left','odue','odid','flags','data'],axis=1)\n",
    "expected_columns_2 = ['id', 'nid', 'ord', 'queue', 'due', 'ivl', 'factor', 'reps','lapses']\n",
    "\n",
    "print_before_after(df_cards.columns.values, df_cards_001_less_cols.columns.values,\"Card Columns:\")\n",
    "\n",
    "assertEquals(lists_equal(df_cards_001_less_cols.columns.values, expected_columns_2), True, \"Card Model Slimmed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Import notes (words) into data frame \"df_notes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take in the 'notes' table, and explicitly save the note id (\"nid\") \n",
    "df_notes = pd.read_sql_query(\"SELECT * FROM notes\", cnx)\n",
    "df_notes = df_notes.rename(columns={'id':'nid'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 395,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertEquals(df_notes.shape[0],8384,\"Rows\") # 2791, 9784, 8403\n",
    "assertEquals(df_notes.shape[1],11,\"Columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Remove (drop) unneeded fields (columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Before: ['nid' 'guid' 'mid' 'mod' 'usn' 'tags' 'flds' 'sfld' 'csum' 'flags' 'data']\n",
      "---------------------------------------------------------------------------\n",
      "After: ['nid' 'mod' 'tags' 'flds']\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_notes_old_col_vals = df_notes.columns.values\n",
    "df_notes = df_notes.drop(['guid','mid','usn','sfld','csum','flags','data'],axis=1)\n",
    "#print(df_notes.columns.values)\n",
    "print_before_after(df_notes_old_col_vals, df_notes.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Split \"fields\" column into multiple, assign field names, drop combined col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_it(func, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    end = time.time()\n",
    "    # https://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python\n",
    "    print(\"{:.0f}\".format((end - start)*1000) + \" miliseconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['nid' 'mod' 'tags' 'Term' 'Yomi1' 'Translation' 'Translation2'\n",
      " 'Translation3' 'AlternateForms' 'PartOfSpeech' 'Sound' 'Sound2' 'Sound3'\n",
      " 'Examples' 'ExamplesAudio' 'AtoQ' 'AtoQaudio' 'AtoQkana'\n",
      " 'AtoQtranslation' 'QandApicture' 'answerPicture' 'Meaning1'\n",
      " 'SimilarWords' 'RelatedWords' 'Breakdown1' 'Comparison' 'Usage' 'Prompt1'\n",
      " 'Prompt2' 'KakuMCD' 'IuMCD' 'ExtraMemo' 'Yomi2' 'Meaning2' 'Breakdown2'\n",
      " 'Picture1' 'Picture2' 'Picture3' 'Picture4' 'HinshiMarker' 'Hint' 'Term2'\n",
      " 'ArabicNumeral' 'CounterKanji' 'Mnemonic' 'SameSoundWords' 'Yomi3'\n",
      " 'gChap' 'gBook' 'semester' 'gNumber' 'Transliteration' 'SoloLookCards'\n",
      " 'TagOverflow' 'blank1' 'blank2']\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(expected_names)-1):\n",
    "    df_notes[expected_names[i]] = df_notes.flds.str.split('\u001f').str.get(i)\n",
    "assertEquals('flds' in df_notes.columns.values, True, \"'flds' Column Found\")\n",
    "df_notes = df_notes.drop(['flds'],axis=1)\n",
    "assertEquals('flds' not in df_notes.columns.values, True, \"'flds' Column Not Found\")\n",
    "print(df_notes.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Check notes for duplicates (shallow check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 399,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertEquals(has_dupes(df_notes), False, \"Duplicates Not Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Check for duplicates by term field in notes data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_dupe_terms(df_in):\n",
    "    location = df_in['Term'].duplicated()\n",
    "    return df_in.loc[location].shape[0] != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assertEquals(has_dupe_terms(df_notes), False, \"Duplicates Found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. Confirm that duplicates dataframe is empty (no dups exist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 56)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dupe = df_notes['Term'].duplicated() #creates list of True/False values\n",
    "print(df_notes[dupe].shape)\n",
    "assertEquals(df_notes[dupe].shape[0], 0, \"Duplicates dataframe is empty.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Inspect an individual card by its term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Postal service\n",
    "def inspect_note(df_in, term):\n",
    "    return df_in[df_in['Term']==term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>mod</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Translation2</th>\n",
       "      <th>Translation3</th>\n",
       "      <th>AlternateForms</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>...</th>\n",
       "      <th>Yomi3</th>\n",
       "      <th>gChap</th>\n",
       "      <th>gBook</th>\n",
       "      <th>semester</th>\n",
       "      <th>gNumber</th>\n",
       "      <th>Transliteration</th>\n",
       "      <th>SoloLookCards</th>\n",
       "      <th>TagOverflow</th>\n",
       "      <th>blank1</th>\n",
       "      <th>blank2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3282</th>\n",
       "      <td>1361674609381</td>\n",
       "      <td>1555887839</td>\n",
       "      <td>Japanese Marked abaCheckNuance addDefinition ...</td>\n",
       "      <td>郵便</td>\n",
       "      <td>ゆうびん</td>\n",
       "      <td>mail, postal service</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>〒</td>\n",
       "      <td>Common word, Noun</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                nid         mod  \\\n",
       "3282  1361674609381  1555887839   \n",
       "\n",
       "                                                   tags Term Yomi1  \\\n",
       "3282   Japanese Marked abaCheckNuance addDefinition ...   郵便  ゆうびん   \n",
       "\n",
       "               Translation Translation2 Translation3 AlternateForms  \\\n",
       "3282  mail, postal service                                        〒   \n",
       "\n",
       "           PartOfSpeech  ... Yomi3 gChap gBook semester gNumber  \\\n",
       "3282  Common word, Noun  ...                                      \n",
       "\n",
       "     Transliteration SoloLookCards TagOverflow blank1 blank2  \n",
       "3282                                                          \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sel1 = inspect_note(df_notes,'郵便')\n",
    "sel1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Point, Commit, Bonfire (for you Souls fans)*\n",
    "\n",
    "At the point in time of the data extraction where the (meta) tag information is made available, we can treat it to both clarify (rename poorly worded tags) & reduce (delete unneeded tags). Since we now have all fields split into their own columns as well, we can treat (modifiy & improve) the columns as well, in a 1-2 process: (1) Fix the tags & (2) Fix the columns\n",
    "*https://en.wikipedia.org/wiki/Souls_(series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shorten_list(takeIn, takeOut):\n",
    "    temp = takeIn.lower().split() # split all the words into a list\n",
    "    temp2 = [word for word in temp if word.lower() not in takeOut] # create a shorter list of words minus the take-outs\n",
    "    return ' '.join(temp2) # return that shorter list as a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_remove_list = ['japanese', 'checkpicture', 'complete', 'haspicture', 'nomemo',\n",
    "                   'researched', 'aaaeditthis', 'addaudio', 'addaudio2', 'addaudioNow',\n",
    "                   'addmore','adjustformatting', 'hascomparison', 'hasmnemonic',\n",
    "                   'customediting','wikidefinition', 'givewill','addaudionow','addprompt',\n",
    "                   'checknuance','giveyaneury','hastextimage', 'marked', 'addpicture',\n",
    "                   'addexampletranslation','basicnumeric', 'genkiplus', 'hasaudio',\n",
    "                   'nativeaudio', 'adddefinition','addexamples', 'addjapaneseprompt',\n",
    "                   'computervoice','haspoliteprefix','nongoo','customdefinition','hashint',\n",
    "                   'abahipriorityfix','kaki','mcd','nobodyknows+','missingwordtype',\n",
    "                   'image','duplicate', 'hasprompt', 'ninshiki','abachecknuance',\n",
    "                   'hasflag','things', 'jim', 'hasunicode', 'editthis','aaahipriority',\n",
    "                   'hassimpledef', 'givecodie', 'forjimmy', 'hasnativeaudio', 'givejimmy2',\n",
    "                   'checkaudio', 'checkwriting', 'hasjlptlevel', 'makekaki', 'checknuance2',\n",
    "                   'checkagain', 'newaudio', 'mail', 'checkexamples','elementaryschool',\n",
    "                   'nvc', 'checkprompt', 'gavejimmy', 'addnativeaudio','checkreading',\n",
    "                   'givecodieapril', 'activated', 'fixformatting','hasplacesuffix',\n",
    "                   'hassuffix','addtranslation','addnewcardtype','addnuance','addtextimage',\n",
    "                   'semicomplete', 'removeroboaudio','fixaudio','hasgramconj', \n",
    "                   'hasquestion', 'addkanji','changenotetype', 'famous', 'challenging',\n",
    "                   'kuverb', 'givwill','karutapoems', 'map', 'hasvisualcomparison',\n",
    "                   'picturekaki', 'jyugemu', '2018', 'type1', 'hasslang', 'apologies',\n",
    "                   'month', 'definitionresearched','soundshift', 'basics1', 'tsuverb',\n",
    "                   'facebook', 'uverb', 'checkfrequency', 'degree', 'hasdefinition',\n",
    "                   'addtransliteration', 'dnd', 'introductions', 'adjustprompt',\n",
    "                   'job', 'particle', 'services', 'mature', 'splitpictures', \n",
    "                   'egaki', 'type5k', 'intimate','extrainfo', 'irregular', 'unlisted',\n",
    "                   'fromwiki', 'checkdifference','addpronunciationdiagram', 'reset',\n",
    "                   'currentevents', 'doubletextimage', 'comparison', 'verbscompoundpast2',\n",
    "                   'attention', 'addmemo', 'averb', 'radio','hasascii', 'fontadjusted',\n",
    "                   'haspronunciation', 'borroweddefinition','alphabet', 'graphics',\n",
    "                   'chiebukuro', 'duolingo', 'ateji', 'fact','type5s', 'fixpicture',\n",
    "                   'politebydefault', 'objects','sensitive', 'groupword', 'addmnemonic',\n",
    "                   'hasmore', 'quote', 'checkformatting','overlap', 'kotobankdef',\n",
    "                   'hasrudeness', 'changedeck', 'specialformatting','yoga',\n",
    "                   'hasjapaneseprompt', 'hasprefix','questionword', 'business', \n",
    "                   'postoffice', 'firstten', 'money', 'robotvoice2', 'ichidan', 'godan',\n",
    "                   'weather','count', 'nodefinition', 'muverb', 'addcomparisonchart', \n",
    "                   'ruverb', 'phone', 'conjugated','haddiv','vulgar','fromkaruta',\n",
    "                   'karutamanual', 'teform', 'qanda', '2019'\n",
    "                  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Remove unneeded tags (meta-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>mod</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Translation2</th>\n",
       "      <th>Translation3</th>\n",
       "      <th>AlternateForms</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>...</th>\n",
       "      <th>Yomi3</th>\n",
       "      <th>gChap</th>\n",
       "      <th>gBook</th>\n",
       "      <th>semester</th>\n",
       "      <th>gNumber</th>\n",
       "      <th>Transliteration</th>\n",
       "      <th>SoloLookCards</th>\n",
       "      <th>TagOverflow</th>\n",
       "      <th>blank1</th>\n",
       "      <th>blank2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1331799797110</td>\n",
       "      <td>1511481489</td>\n",
       "      <td>Japanese Marked abaCheckNuance checkNuance co...</td>\n",
       "      <td>臨機応変</td>\n",
       "      <td>りんきおうへん</td>\n",
       "      <td>adapting oneself to the requirements of the mo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Noun, No-adjective</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1331799797112</td>\n",
       "      <td>1511481489</td>\n",
       "      <td>Japanese complete noMemo researched wwwjdic</td>\n",
       "      <td>隙間</td>\n",
       "      <td>すきま</td>\n",
       "      <td>&lt;div&gt;crevice; crack; gap; opening&lt;/div&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;div&gt;Common word, Noun&lt;/div&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1331799797113</td>\n",
       "      <td>1511481489</td>\n",
       "      <td>Japanese Marked abaCheckNuance checkNuance co...</td>\n",
       "      <td>苦汁</td>\n",
       "      <td>にがり</td>\n",
       "      <td>bittern; concentrated solution of salts (esp. ...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Noun</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             nid         mod  \\\n",
       "0  1331799797110  1511481489   \n",
       "1  1331799797112  1511481489   \n",
       "2  1331799797113  1511481489   \n",
       "\n",
       "                                                tags  Term    Yomi1  \\\n",
       "0   Japanese Marked abaCheckNuance checkNuance co...  臨機応変  りんきおうへん   \n",
       "1       Japanese complete noMemo researched wwwjdic     隙間      すきま   \n",
       "2   Japanese Marked abaCheckNuance checkNuance co...    苦汁      にがり   \n",
       "\n",
       "                                         Translation Translation2  \\\n",
       "0  adapting oneself to the requirements of the mo...                \n",
       "1            <div>crevice; crack; gap; opening</div>                \n",
       "2  bittern; concentrated solution of salts (esp. ...                \n",
       "\n",
       "  Translation3 AlternateForms                  PartOfSpeech  ... Yomi3 gChap  \\\n",
       "0                                        Noun, No-adjective  ...               \n",
       "1                              <div>Common word, Noun</div>  ...               \n",
       "2                                                      Noun  ...               \n",
       "\n",
       "  gBook semester gNumber Transliteration SoloLookCards TagOverflow blank1  \\\n",
       "0                                                                           \n",
       "1                                                                           \n",
       "2                                                                           \n",
       "\n",
       "  blank2  \n",
       "0         \n",
       "1         \n",
       "2         \n",
       "\n",
       "[3 rows x 56 columns]"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# survey a few notes to see example tag data\n",
    "df_notes.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Tags for 臨機応変\n",
      "---------------------------------------------------------------------------\n",
      "Before:  Japanese Marked abaCheckNuance checkNuance complete noMemo researched wwwjdic yojijukugo \n",
      "---------------------------------------------------------------------------\n",
      "After: wwwjdic yojijukugo\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# likely useful tags: katakana, Waseigo, Food, Phrases, casual, restaurant, travel, commonWord, noun, suruVerb\n",
    "\n",
    "df_notes_001_less_tags = df_notes.copy() #originally \"df_notes_less_tags\"\n",
    "df_notes_001_less_tags['tags'] = df_notes_001_less_tags['tags'].apply(lambda x: shorten_list(str(x), tag_remove_list))\n",
    "\n",
    "print_before_after(df_notes['tags'].iloc[0], df_notes_001_less_tags['tags'].iloc[0],\"Tags for \" + df_notes['Term'].iloc[0])\n",
    "\n",
    "assertEquals(\"Japanese\" in df_notes['tags'].iloc[0].split(), True, \"Contains Tag 'Japanese'\")\n",
    "assertEquals(\"Japanese\" in df_notes_001_less_tags['tags'].iloc[0].split(), False, \"Contains Tag 'Japanese'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 17. Rename useful tags (meta-data) that were poorly named"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace list (formerly named 'tag_replace_list')\n",
    "tag_rename_dict = {\n",
    "    'aalowfrequency':'rare checked', 'aatechnical':'technical checked', 'aaanonkaiwa':'nonconvo checked',\n",
    "    'wwwjdic':'fromdict', 'expression':'phrases', 'numberonly':'number',\n",
    "    'grammarpoint':'grammar', 'jisho':'fromdict', 'pointingword':'directions',\n",
    "    'geometry':'math technical', 'genki':'textbook', 'jpn202':'college',\n",
    "    'jpn201':'college', 'jpn101':'college', 'jpn102':'college', 'kentei':'fromexam',\n",
    "    'proficiencytest':'fromexam', 'bodypart':'body', '5kyuu':'fromexam',\n",
    "    'linguisticreference':'technical', 'conversation':'convo',\n",
    "    'fromconvo':'convo', 'culturepoint':'culture', 'checkednuance':'checked',\n",
    "    'checkedpictures':'checked', 'checkednuance':'checked', 'medical':'technical',\n",
    "    'anatomy':'body', 'places':'place', 'animals':'animal',\n",
    "    'newspaperterm':'fromnewspaper', 'checkedreading':'checked',\n",
    "    'abbreviation':'abbr','firstsemester':'semester1','onecharacter':'len1',\n",
    "    'sentence':'phrase', 'verbs':'verb', 'convook':'checked convo','inuse':'checked',\n",
    "    'nuancechecked':'checked','insects':'animal insect','sightseeing':'travel',\n",
    "    'accessories':'clothing', 'grammarsuffix':'suffix', 'oceanlife':'animal ocean',\n",
    "    'science':'technical', 'written':'nonconvo', 'notrare':'checked',\n",
    "    'aajoke':'silly', 'intonationcompare':'hassimilar', 'ij':'textbook',\n",
    "    'goodcard':'inspect','aahilevel':'challenging inspect', 'ijvocab':'textbook',\n",
    "    'cliothing':'clothing','unused':'nonconvo rare checked',\n",
    "    'aaunused':'nonconvo rare checked', 'samesound':'hassame','animals':'animal',\n",
    "    'dictionary':'fromdict','usuallywritteninkana':'kana',\n",
    "    'abVeryRare':'rare checked', 'yojijukugo':'rare idiom', 'abcasual':'casual checked convo',\n",
    "    'literaryform':'nonconvo', 'onomatopoeiclike':'onomatopoeic','kenjo':'humble',\n",
    "    'colors':'color', 'forest':'nature','flower':'plant nature', 'aaok':'checked',\n",
    "    'questions': 'question', 'adverbs':'adverb','book2':'textbook',\n",
    "    'book1':'textbook','proficiencytest':'fromtest','animalscomplete':'animal',\n",
    "    'sonkei':'respectful','eating':'food','fruit':'food','neverused':'nonconvo rare',\n",
    "    'domainspecific':'technical','seaons':'season','seasons':'season',\n",
    "    'prefecture':'place','plantpart':'plant', \"hakataben\":\"dialect\", \"fish\":\"animal fish\",\n",
    "    \"transitive\":\"transitive verb\", \"intransitive\":\"intransitive verb\",\n",
    "    \"aaunecessary\":\"nonconvo checked\", \"vegetables\":\"vegetable food plant\",\n",
    "    \"counters\":\"counter\", \"senmonyougo\":\"technical\", \"countries\":\"country place\",\n",
    "    \"date\":\"datesandtime\", \"rarelyused\":\"rare\", \"aaakaiwa\":\"convo checked\", \"cool\":\"inspect\",\n",
    "    \"investigate\":\"inspect\"\n",
    "}\n",
    "\n",
    "#todo: investigate:\n",
    "#editformatting,  datesandtime, linguistics, reference, adult, adjustpicture, checkpronunciation, addhint, challenging, inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_list(takeIn, replaceDict):\n",
    "    temp = takeIn.lower().split()\n",
    "    temp2 = []\n",
    "    for word in temp:\n",
    "        if word in replaceDict:\n",
    "            temp2.append(replaceDict.get(word)) # if the word exists in the dictionary, replace it\n",
    "        else:\n",
    "            temp2.append(word) # if the word doesnt't exist in the dictionary, leave it alone\n",
    "    return ' '.join(temp2) # return that shorter list as a string\n",
    "\n",
    "# inspect further:\n",
    "# multiwriting, multimeaning, multipicture, multiterm, multireading, mergeterms, checkpronunciation, customterm,\n",
    "# goodcard, personalized, silly, addjlptlevel, checkpronunciation, mergeterms, customterm, transportation vs travel\n",
    "\n",
    "# categorize: iadjective, naajective, verb, counter, commonword, suruverb, pronoun, question, phrases, kuverb, godan, ichidan, intransitive, transitive, noun, adverbialnoun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Tags for 臨機応変\n",
      "---------------------------------------------------------------------------\n",
      "Before: wwwjdic yojijukugo\n",
      "---------------------------------------------------------------------------\n",
      "After: fromdict rare idiom\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_002_better_tags = df_notes_001_less_tags.copy() # originally \"df_notes_better_tags\"\n",
    "df_notes_002_better_tags['tags'] = df_notes_002_better_tags['tags'].apply(lambda x: replace_list(str(x), tag_rename_dict))\n",
    "\n",
    "print_before_after(df_notes_001_less_tags['tags'].iloc[0], df_notes_002_better_tags['tags'].iloc[0], \"Tags for \" + df_notes_002_better_tags['Term'].iloc[0])\n",
    "\n",
    "assertEquals(\"wwwjdic\" in df_notes_001_less_tags['tags'].iloc[0].split(), True, \"Contains Tag 'wwwjdic'\")\n",
    "assertEquals(\"wwwjdic\" in df_notes_002_better_tags['tags'].iloc[0].split(), False, \"Contains Tag 'wwwjdic'\")\n",
    "assertEquals(\"fromdict\" in df_notes_001_less_tags['tags'].iloc[0].split(), False, \"Contains Tag 'fromdict'\")\n",
    "assertEquals(\"fromdict\" in df_notes_002_better_tags['tags'].iloc[0].split(), True, \"Contains Tag 'fromdict'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                                       1622\n",
       "fromdict                                796\n",
       "fromtest textbook                       600\n",
       "textbook textbook                       453\n",
       "college textbook textbook               241\n",
       "verb                                    200\n",
       "fromdict verb                           144\n",
       "fromexam                                126\n",
       "len1                                    122\n",
       "hiragana college textbook textbook      107\n",
       "counter numeric                          97\n",
       "numeric                                  81\n",
       "addsimilar                               81\n",
       "fromdict media                           72\n",
       "college textbook semester1 textbook      71\n",
       "fromexam textbook                        65\n",
       "fromdict lyrics                          63\n",
       "convo                                    61\n",
       "n3 fromdict transitive verb verb         58\n",
       "college textbook textbook katakana       54\n",
       "Name: tags, dtype: int64"
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_002_better_tags['tags'].value_counts()[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can attempt to inspect which tags are most common, in which combinations, and which words would be ideal\n",
    "for further additional metadata. However, **our tags are still lumped together** at this point. Also, there is\n",
    "reason to believe that **some tags are showing up multiple times in the same tag string**. In order to properly count tag frequency, the duplicates must be confirmed absent (ie. found & removed). Then, the occurance (word frequency) of each tag may then be summed up for the tags column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 18. Inspect a note that you suspect has tag duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_note_by_id(df_in, nid):\n",
    "    return df_in[df_in['nid']==nid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm that a particular note has tag duplicates\n",
    "# crimison note id: 1369286386384\n",
    "note_id_1 = 1369286386384\n",
    "assertEquals(inspect_note_by_id(df_notes_002_better_tags,note_id_1).tags.values[0],\"fromexam color fromexam len1\",\"Four tags total with two duplicates exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>mod</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Translation2</th>\n",
       "      <th>Translation3</th>\n",
       "      <th>AlternateForms</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>...</th>\n",
       "      <th>Yomi3</th>\n",
       "      <th>gChap</th>\n",
       "      <th>gBook</th>\n",
       "      <th>semester</th>\n",
       "      <th>gNumber</th>\n",
       "      <th>Transliteration</th>\n",
       "      <th>SoloLookCards</th>\n",
       "      <th>TagOverflow</th>\n",
       "      <th>blank1</th>\n",
       "      <th>blank2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3847</th>\n",
       "      <td>1369286386384</td>\n",
       "      <td>1511481489</td>\n",
       "      <td>fromexam color fromexam len1</td>\n",
       "      <td>紅</td>\n",
       "      <td>くれない</td>\n",
       "      <td>&lt;div&gt;deep red; crimson&lt;/div&gt;&lt;div&gt;&lt;br /&gt;&lt;/div&gt;&lt;...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;div&gt;Common word, Noun&lt;/div&gt;&lt;div&gt;Common word, ...</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                nid         mod                          tags Term Yomi1  \\\n",
       "3847  1369286386384  1511481489  fromexam color fromexam len1    紅  くれない   \n",
       "\n",
       "                                            Translation Translation2  \\\n",
       "3847  <div>deep red; crimson</div><div><br /></div><...                \n",
       "\n",
       "     Translation3 AlternateForms  \\\n",
       "3847                               \n",
       "\n",
       "                                           PartOfSpeech  ... Yomi3 gChap  \\\n",
       "3847  <div>Common word, Noun</div><div>Common word, ...  ...               \n",
       "\n",
       "     gBook semester gNumber Transliteration SoloLookCards TagOverflow blank1  \\\n",
       "3847                                                                           \n",
       "\n",
       "     blank2  \n",
       "3847         \n",
       "\n",
       "[1 rows x 56 columns]"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example of item with tag duplication\n",
    "sel2 = inspect_note_by_id(df_notes_002_better_tags,note_id_1)\n",
    "sel2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 19. Remove duplicate tags (convert tag strings > lists > sets > strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converts a tag string to a list to a set back to a string (this removes the duplicates)\n",
    "def remove_dupes(t):\n",
    "    temp = list(set(t.lower().split()))\n",
    "    return ' '.join(temp) # return as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_003_tags_no_dups = df_notes_002_better_tags.copy()\n",
    "df_notes_003_tags_no_dups['tags'] = df_notes_003_tags_no_dups['tags'].apply(lambda x: remove_dupes(str(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determines if an individual tag substring exists in a larger tags list string\n",
    "def tag_exists(tags, tag):\n",
    "    return 1 if tag in tags.split() else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color len1 fromexam\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inspect_note_by_id(df_notes_003_tags_no_dups,note_id_1).tags.values[0])\n",
    "assertEquals(tag_exists(inspect_note_by_id(df_notes_003_tags_no_dups,note_id_1).tags.values[0],\"len1\"), 1, \"tag 'len1' remains\")\n",
    "assertEquals(tag_exists(inspect_note_by_id(df_notes_003_tags_no_dups,note_id_1).tags.values[0],\"fromexam\"), 1, \"tag 'fromexam' remains\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears we have most, if not all, of the data we need to start. The format of the dates though is not yet human readable. Let's fix that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20. Convert (& preserve) note ID to note creation date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Term 臨機応変\n",
      "---------------------------------------------------------------------------\n",
      "Before: 1331799797110\n",
      "---------------------------------------------------------------------------\n",
      "After: 2012-03-15\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dueNum = 782 # this represents days from collection creation date\n",
    "#crt = 1357635600 # this represents the collection creation date #todo: query dynamically from database\n",
    "#print(\"mid 'model id': \" + time.ctime(int(\"1768161991\"))) # 1 day = 86400 seconds\n",
    "\n",
    "df_notes_004_with_date = df_notes_003_tags_no_dups.copy()\n",
    "df_notes_004_with_date['NoteCreated']= pd.to_datetime(df_notes_004_with_date['nid'],unit='ms')\n",
    "df_notes_004_with_date['NoteCreated'] = df_notes_004_with_date['NoteCreated'].dt.date\n",
    "df_notes_004_with_date.head()\n",
    "\n",
    "print_before_after(df_notes_003_tags_no_dups['nid'].iloc[0], df_notes_004_with_date['NoteCreated'].iloc[0],\"Term \" + df_notes_004_with_date['Term'].iloc[0])\n",
    "\n",
    "assertEquals(df_notes_004_with_date['nid'].iloc[0], 1331799797110, \"Note ID is in Epoch Units\")\n",
    "assertEquals(str(df_notes_004_with_date['NoteCreated'].iloc[0]), \"2012-03-15\", \"Note ID is in datetime date format year-month-day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. Generate Note Last Modified Date from \"Mod\" ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_005_last_modified = df_notes_004_with_date.copy()\n",
    "df_notes_005_last_modified['mod'] = pd.to_datetime(df_notes_005_last_modified['mod'],unit='s')\n",
    "df_notes_005_last_modified['mod'] = df_notes_005_last_modified['mod'].dt.date\n",
    "\n",
    "assertEquals(str(df_notes_005_last_modified['mod'].iloc[0]), \"2017-11-23\", \"Note last modified is in datetime date format year-month-day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Create df_notes_final data frame for export & further usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_final = df_notes_005_last_modified.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Export df_notes_section_2_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_final.to_csv('datasets/notes_section_2_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24. Generate Card Creation Date from Card ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 424,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards_002_created_date = df_cards_001_less_cols.copy()\n",
    "df_cards_002_created_date['CardCreated'] = pd.to_datetime(df_cards_002_created_date['id'],unit='ms')\n",
    "df_cards_002_created_date['CardCreated'] = df_cards_002_created_date['CardCreated'].dt.date\n",
    "\n",
    "assertEquals(str(df_cards_002_created_date['CardCreated'].iloc[0]), \"2012-03-15\", \"Card ID is in datetime date format year-month-day\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Card Rows:\n",
      "---------------------------------------------------------------------------\n",
      "Before: 19315\n",
      "---------------------------------------------------------------------------\n",
      "After: 8231\n",
      "---------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nid</th>\n",
       "      <th>ord</th>\n",
       "      <th>queue</th>\n",
       "      <th>due</th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18909</th>\n",
       "      <td>2019-02-06</td>\n",
       "      <td>1549184119039</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2245</td>\n",
       "      <td>10</td>\n",
       "      <td>2410</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18910</th>\n",
       "      <td>2019-02-17</td>\n",
       "      <td>1550402953788</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2232</td>\n",
       "      <td>1</td>\n",
       "      <td>2410</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18911</th>\n",
       "      <td>2019-02-17</td>\n",
       "      <td>1550402953788</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2236</td>\n",
       "      <td>1</td>\n",
       "      <td>2210</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18912</th>\n",
       "      <td>2019-02-17</td>\n",
       "      <td>1550403040864</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2232</td>\n",
       "      <td>1</td>\n",
       "      <td>2410</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18913</th>\n",
       "      <td>2019-02-17</td>\n",
       "      <td>1550403040864</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2240</td>\n",
       "      <td>5</td>\n",
       "      <td>2410</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id            nid  ord  queue   due  ivl  factor  reps  lapses\n",
       "18909  2019-02-06  1549184119039    2      2  2245   10    2410     2       0\n",
       "18910  2019-02-17  1550402953788    0      2  2232    1    2410     3       0\n",
       "18911  2019-02-17  1550402953788    2      2  2236    1    2210     5       1\n",
       "18912  2019-02-17  1550403040864    0      2  2232    1    2410     2       0\n",
       "18913  2019-02-17  1550403040864    2      2  2240    5    2410     3       0"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#queue           integer not null,\n",
    "#      -- -3=sched buried, -2=user buried, -1=suspended,\n",
    "#      -- 0=new, 1=learning, 2=due (as for type)\n",
    "\n",
    "df_cards_003_no_new = df_cards_002_created_date.copy()\n",
    "df_cards_003_no_new = df_cards_003_no_new[df_cards_003_no_new['queue']!=0] # remove cards marked as new\n",
    "df_cards_003_no_new = df_cards_003_no_new[df_cards_003_no_new['reps']!=0] # remove cards that have not been reviewed\n",
    "df_cards_003_no_new = df_cards_003_no_new[df_cards_003_no_new['queue']!=-1] # remove cards that are currently suspended\n",
    "# https://stackoverflow.com/questions/18196203/how-to-conditionally-update-dataframe-column-in-pandas\n",
    "df_cards_003_no_new.loc[df_cards_003_no_new['due'] > 10000, 'due'] = 0 # assign 0 to the due # todo: update w/ last studied date from revlog # todo: comment this line out once you have updated the collection import\n",
    "\n",
    "print_before_after(df_cards_002_created_date.shape[0], df_cards_003_no_new.shape[0],\"Card Rows:\")\n",
    "\n",
    "df_cards_003_no_new.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>nid</th>\n",
       "      <th>ord</th>\n",
       "      <th>queue</th>\n",
       "      <th>due</th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3366</th>\n",
       "      <td>2013-03-11</td>\n",
       "      <td>1362961413265</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1300</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11245</th>\n",
       "      <td>2017-01-21</td>\n",
       "      <td>1483483650784</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2160</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11274</th>\n",
       "      <td>2017-02-26</td>\n",
       "      <td>1346220179900</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1760</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               id            nid  ord  queue  due  ivl  factor  reps  lapses\n",
       "3366   2013-03-11  1362961413265    0      1    0    1    1300    18       1\n",
       "11245  2017-01-21  1483483650784    4      1    0    1    2160     8       1\n",
       "11274  2017-02-26  1346220179900    4      1    0    1    1760    12       3"
      ]
     },
     "execution_count": 426,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# todo: remove this cell once the newest collection has been imported\n",
    "# confirm that the three cards \"in learning\" have their due dates reset back to 0 for date transformation\n",
    "sel3 = df_cards_003_no_new[df_cards_003_no_new['due'] == 0]\n",
    "sel3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 25. Generate Due Date from Due Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OK'"
      ]
     },
     "execution_count": 427,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards_004_due_date = df_cards_003_no_new.copy()\n",
    "df_cards_004_due_date['due'] = pd_crt + df_cards_004_due_date['due'].map(timedelta)\n",
    "df_cards_004_due_date['due'] = df_cards_004_due_date['due'].dt.date\n",
    "\n",
    "assertEquals(str(df_cards_004_due_date['due'].iloc[0]), \"2015-03-08\", \"Card due date is in datetime date format year-month-day\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Create df_cards_final data frame for export & further usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cards_final = df_cards_004_due_date.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 27. Export df_cards_section_2_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cards_final.to_csv('datasets/cards_section_2_final.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 28. Merge card & note data frames to conduct cross analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8231, 65)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>mod</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Translation2</th>\n",
       "      <th>Translation3</th>\n",
       "      <th>AlternateForms</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>...</th>\n",
       "      <th>blank2</th>\n",
       "      <th>NoteCreated</th>\n",
       "      <th>id</th>\n",
       "      <th>ord</th>\n",
       "      <th>queue</th>\n",
       "      <th>due</th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1331799797110</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>idiom fromdict rare</td>\n",
       "      <td>臨機応変</td>\n",
       "      <td>りんきおうへん</td>\n",
       "      <td>adapting oneself to the requirements of the mo...</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Noun, No-adjective</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-03-08</td>\n",
       "      <td>65</td>\n",
       "      <td>1680</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1331799797112</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>fromdict</td>\n",
       "      <td>隙間</td>\n",
       "      <td>すきま</td>\n",
       "      <td>&lt;div&gt;crevice; crack; gap; opening&lt;/div&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;div&gt;Common word, Noun&lt;/div&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-03-03</td>\n",
       "      <td>149</td>\n",
       "      <td>2080</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1331799797114</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>fromdict</td>\n",
       "      <td>移籍</td>\n",
       "      <td>いせき</td>\n",
       "      <td>&lt;div&gt;changing household registry; transfer (e....</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>&lt;div&gt;Common word, Noun, Suru verb&lt;/div&gt;</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-02-04</td>\n",
       "      <td>99</td>\n",
       "      <td>1980</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1331799797117</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>fromdict verb</td>\n",
       "      <td>吊るす</td>\n",
       "      <td>つるす</td>\n",
       "      <td>to hang</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-03-17</td>\n",
       "      <td>143</td>\n",
       "      <td>2130</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1331799797118</td>\n",
       "      <td>2017-11-23</td>\n",
       "      <td>convo checked fromdict</td>\n",
       "      <td>和やか</td>\n",
       "      <td>なごやか</td>\n",
       "      <td>harmonious, peaceful</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>2012-03-15</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-02-06</td>\n",
       "      <td>74</td>\n",
       "      <td>1880</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             nid         mod                    tags  Term    Yomi1  \\\n",
       "0  1331799797110  2017-11-23     idiom fromdict rare  臨機応変  りんきおうへん   \n",
       "1  1331799797112  2017-11-23                fromdict    隙間      すきま   \n",
       "2  1331799797114  2017-11-23                fromdict    移籍      いせき   \n",
       "3  1331799797117  2017-11-23           fromdict verb   吊るす      つるす   \n",
       "4  1331799797118  2017-11-23  convo checked fromdict   和やか     なごやか   \n",
       "\n",
       "                                         Translation Translation2  \\\n",
       "0  adapting oneself to the requirements of the mo...                \n",
       "1            <div>crevice; crack; gap; opening</div>                \n",
       "2  <div>changing household registry; transfer (e....                \n",
       "3                                            to hang                \n",
       "4                               harmonious, peaceful                \n",
       "\n",
       "  Translation3 AlternateForms                             PartOfSpeech  ...  \\\n",
       "0                                                   Noun, No-adjective  ...   \n",
       "1                                         <div>Common word, Noun</div>  ...   \n",
       "2                              <div>Common word, Noun, Suru verb</div>  ...   \n",
       "3                                                                       ...   \n",
       "4                                                                       ...   \n",
       "\n",
       "  blank2 NoteCreated          id ord queue         due  ivl factor reps lapses  \n",
       "0         2012-03-15  2012-03-15   0     2  2015-03-08   65   1680   10      1  \n",
       "1         2012-03-15  2012-03-15   0     2  2015-03-03  149   2080    8      1  \n",
       "2         2012-03-15  2012-03-15   0     2  2015-02-04   99   1980    7      0  \n",
       "3         2012-03-15  2012-03-15   0     2  2015-03-17  143   2130    6      1  \n",
       "4         2012-03-15  2012-03-15   0     2  2015-02-06   74   1880   15      3  \n",
       "\n",
       "[5 rows x 65 columns]"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now that we have note id's for all the words, we can\n",
    "# join together these separate dataframes\n",
    "df_combo = pd.merge(df_notes_final, df_cards_final, on='nid')\n",
    "print(df_combo.shape)\n",
    "df_combo.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that card types are being rendered as numbers, which makes it less human readible. We will fix this. Additionally, our card model has a bunch of columns (fields) with no values in them, whatsoever. These can be taken out for the data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_blank (s):\n",
    "    return not (s and s.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frame_of_cards_by_term(df, t):\n",
    "    return df.loc[df['Term']==t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>mod</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>Translation</th>\n",
       "      <th>Translation2</th>\n",
       "      <th>Translation3</th>\n",
       "      <th>AlternateForms</th>\n",
       "      <th>PartOfSpeech</th>\n",
       "      <th>...</th>\n",
       "      <th>blank2</th>\n",
       "      <th>NoteCreated</th>\n",
       "      <th>id</th>\n",
       "      <th>ord</th>\n",
       "      <th>queue</th>\n",
       "      <th>due</th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2893</th>\n",
       "      <td>1354094556789</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>n3 fromtest noun commonword textbook suruverb</td>\n",
       "      <td>発明</td>\n",
       "      <td>はつめい</td>\n",
       "      <td>&lt;span style=\"\"&gt;&lt;div&gt;invention&lt;/div&gt;&lt;/span&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Common word, Noun, Suru verb</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2015-06-30</td>\n",
       "      <td>80</td>\n",
       "      <td>1300</td>\n",
       "      <td>73</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2894</th>\n",
       "      <td>1354094556789</td>\n",
       "      <td>2018-12-03</td>\n",
       "      <td>n3 fromtest noun commonword textbook suruverb</td>\n",
       "      <td>発明</td>\n",
       "      <td>はつめい</td>\n",
       "      <td>&lt;span style=\"\"&gt;&lt;div&gt;invention&lt;/div&gt;&lt;/span&gt;</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Common word, Noun, Suru verb</td>\n",
       "      <td>...</td>\n",
       "      <td></td>\n",
       "      <td>2012-11-28</td>\n",
       "      <td>2013-06-21</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-10-24</td>\n",
       "      <td>1056</td>\n",
       "      <td>1300</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                nid         mod  \\\n",
       "2893  1354094556789  2018-12-03   \n",
       "2894  1354094556789  2018-12-03   \n",
       "\n",
       "                                               tags Term Yomi1  \\\n",
       "2893  n3 fromtest noun commonword textbook suruverb   発明  はつめい   \n",
       "2894  n3 fromtest noun commonword textbook suruverb   発明  はつめい   \n",
       "\n",
       "                                     Translation Translation2 Translation3  \\\n",
       "2893  <span style=\"\"><div>invention</div></span>                             \n",
       "2894  <span style=\"\"><div>invention</div></span>                             \n",
       "\n",
       "     AlternateForms                  PartOfSpeech  ... blank2 NoteCreated  \\\n",
       "2893                 Common word, Noun, Suru verb  ...         2012-11-28   \n",
       "2894                 Common word, Noun, Suru verb  ...         2012-11-28   \n",
       "\n",
       "              id ord queue         due   ivl factor reps lapses  \n",
       "2893  2012-11-28   0     2  2015-06-30    80   1300   73      9  \n",
       "2894  2013-06-21   4     2  2021-10-24  1056   1300   20      0  \n",
       "\n",
       "[2 rows x 65 columns]"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's look a a small slice of data, to infer what we may\n",
    "# we can take a broad overview look at the dataset to more quickly isolate candidates for removal\n",
    "s = get_frame_of_cards_by_term(df_combo, '発明')\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 29. Determine which columns (fields) are unused & can be safely removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removal candidates: ['Sound3', 'AtoQ', 'AtoQaudio', 'AtoQkana', 'AtoQtranslation', 'QandApicture', 'answerPicture', 'blank1', 'blank2']\n"
     ]
    }
   ],
   "source": [
    "col_names = df_combo.columns.values\n",
    "#print(is_blank(df_combo['Translation2'].iloc[0])) # see that this cell for this row is indeed blank\n",
    "\n",
    "row_cnt = df_combo.shape[0] # number of rows in df_combo\n",
    "\n",
    "# https://stackoverflow.com/questions/49677060/pandas-count-empty-strings-in-a-column\n",
    "empty_strings = pd.DataFrame(df_combo.values == '',columns=col_names) # find all empty strings in a DataFrame\n",
    "temp_dict = (empty_strings.sum()).to_dict()  # save the location of all empty strings as a DataFrame of booleans\n",
    "removal_candidates = []\n",
    "for key in temp_dict.items():\n",
    "    if key[1] == row_cnt:\n",
    "        removal_candidates.append(key[0])\n",
    "print(\"Removal candidates:\", removal_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 30. Trim unneeded (empty) columns from combo data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "Before: (8231, 65)\n",
      "---------------------------------------------------------------------------\n",
      "After: (8231, 55)\n",
      "---------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------\n",
      "Before: ['nid' 'mod' 'tags' 'Term' 'Yomi1' 'Translation' 'Translation2'\n",
      " 'Translation3' 'AlternateForms' 'PartOfSpeech' 'Sound' 'Sound2' 'Sound3'\n",
      " 'Examples' 'ExamplesAudio' 'AtoQ' 'AtoQaudio' 'AtoQkana'\n",
      " 'AtoQtranslation' 'QandApicture' 'answerPicture' 'Meaning1'\n",
      " 'SimilarWords' 'RelatedWords' 'Breakdown1' 'Comparison' 'Usage' 'Prompt1'\n",
      " 'Prompt2' 'KakuMCD' 'IuMCD' 'ExtraMemo' 'Yomi2' 'Meaning2' 'Breakdown2'\n",
      " 'Picture1' 'Picture2' 'Picture3' 'Picture4' 'HinshiMarker' 'Hint' 'Term2'\n",
      " 'ArabicNumeral' 'CounterKanji' 'Mnemonic' 'SameSoundWords' 'Yomi3'\n",
      " 'gChap' 'gBook' 'semester' 'gNumber' 'Transliteration' 'SoloLookCards'\n",
      " 'TagOverflow' 'blank1' 'blank2' 'NoteCreated' 'id' 'ord' 'queue' 'due'\n",
      " 'ivl' 'factor' 'reps' 'lapses']\n",
      "---------------------------------------------------------------------------\n",
      "After: ['nid' 'mod' 'tags' 'Term' 'Yomi1' 'Translation' 'Translation2'\n",
      " 'Translation3' 'AlternateForms' 'PartOfSpeech' 'Sound' 'Sound2'\n",
      " 'Examples' 'ExamplesAudio' 'Meaning1' 'SimilarWords' 'RelatedWords'\n",
      " 'Breakdown1' 'Comparison' 'Usage' 'Prompt1' 'Prompt2' 'KakuMCD' 'IuMCD'\n",
      " 'ExtraMemo' 'Yomi2' 'Meaning2' 'Breakdown2' 'Picture1' 'Picture2'\n",
      " 'Picture3' 'Picture4' 'HinshiMarker' 'Hint' 'Term2' 'ArabicNumeral'\n",
      " 'CounterKanji' 'Mnemonic' 'SameSoundWords' 'Yomi3' 'gChap' 'gBook'\n",
      " 'semester' 'gNumber' 'Transliteration' 'SoloLookCards' 'TagOverflow'\n",
      " 'NoteCreated' 'id' 'ord' 'due' 'ivl' 'factor' 'reps' 'lapses']\n",
      "---------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "df_combo_001_less_cols = df_combo.copy()\n",
    "\n",
    "removal_list = list(removal_candidates + ['queue'])\n",
    "\n",
    "df_combo_001_less_cols = df_combo_001_less_cols.drop(removal_list,axis=1)\n",
    "\n",
    "print_before_after(df_combo.shape, df_combo_001_less_cols.shape)\n",
    "print_before_after(df_combo.columns.values, df_combo_001_less_cols.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 31. Label card types by their names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     6842\n",
      "4     1109\n",
      "7      267\n",
      "2       11\n",
      "11       2\n",
      "Name: ord, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "read      6842\n",
       "look      1109\n",
       "listen     267\n",
       "recall      11\n",
       "Name: ord, dtype: int64"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ord stands for 'ordinal' : identifies which of the card templates it corresponds to\n",
    "print(df_combo_001_less_cols['ord'].value_counts()) # these are the card vectors\n",
    "\n",
    "# since our dataset contains a single card of a single card vector, & the card vectors\n",
    "# aren't named/labeled, let's remove the outlier & add the names\n",
    "df_combo_002_types_labeled = df_combo_001_less_cols.copy()\n",
    "df_combo_002_types_labeled = df_combo_002_types_labeled.drop(df_combo_002_types_labeled[df_combo_002_types_labeled['ord'] == 11].index)\n",
    "\n",
    "df_combo_002_types_labeled['ord'].value_counts() # the check shall pass\n",
    "\n",
    "# now, to map the names onto the card vectors\n",
    "df_combo_002_types_labeled['ord'] = df_combo_002_types_labeled['ord'].map({0:'read', 2:'recall',4:'look',7:'listen'})\n",
    "df_combo_002_types_labeled['ord'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 32. Create binary exists/not columns based on presence of a given tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_column_by_tag(df, tag):\n",
    "    df[tag] = df['tags'].apply(lambda x: tag_exists(str(x), tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_003_with_binary = df_combo_002_types_labeled.copy()\n",
    "inspect_list = [\"commonword\", \"clothing\", \"animal\", \"body\", \"food\", \"place\",\n",
    "                \"textbook\", \"college\", \"fromdict\", \"fromexam\",\n",
    "                \"len1\", \"n1\", \"n2\", \"n3\", \"n4\", \"n5\"\n",
    "               ]\n",
    "for item in inspect_list:\n",
    "    add_column_by_tag(df_combo_003_with_binary, item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "object    50\n",
       "int64     21\n",
       "dtype: int64"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combo_003_with_binary.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 33. Create interval quartile sections for visualization purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qcut: Quantile-based discretization function. Discretize variable into equal-sized buckets\n",
    "# based on rank or based on sample quantiles. For example 1000 values for 10 quantiles would\n",
    "# produce a Categorical object indicating quantile membership for each data point.\n",
    "# http://www.datasciencemadesimple.com/quantile-decile-rank-column-pandas-python-2/\n",
    "df_combo_003_with_binary['ivl_q'] = pd.qcut(df_combo_003_with_binary['ivl'],5,labels=False)\n",
    "df_combo_003_with_binary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further refine the dataframe entries to represent which notes have (1) visual data, (2) audio data, and (3) a L1 (\"first language\", English in this case) translation. We can represent these with binary values (0 for doesn't exist, 1 for exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 35. Create boolean columns for predictive models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laura calls this process \"Data Enriching\"\n",
    "intify_list = ['hasPOS','hasVisual','hasAudio','hasMultiMeaning','hasMultiReading','hasSimilar','hasHomophone','hasAltForm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/17383094/how-can-i-map-true-false-to-1-0-in-a-pandas-dataframe\n",
    "df_combo_003_with_binary['hasPOS'] = df_combo_003_with_binary['PartOfSpeech']!=\"\" #todo: expand upon this, by tagify\n",
    "df_combo_003_with_binary['hasVisual'] = df_combo_003_with_binary['Picture1']!=\"\"\n",
    "df_combo_003_with_binary['hasAudio'] = df_combo_003_with_binary['Sound']!=\"\"\n",
    "df_combo_003_with_binary['hasMultiMeaning'] = df_combo_003_with_binary['Translation2' and 'Translation3' and 'Meaning2']!=\"\"\n",
    "df_combo_003_with_binary['hasMultiReading'] = df_combo_003_with_binary['Yomi2']!=\"\" # todo: inspect & incorporate venn diagram: https://commons.wikimedia.org/wiki/File:Homograph_homophone_venn_diagram.png\n",
    "df_combo_003_with_binary['hasSimilar'] = df_combo_003_with_binary['SimilarWords']!=\"\"\n",
    "df_combo_003_with_binary['hasHomophone'] = df_combo_003_with_binary['SameSoundWords']!=\"\" # write function, detect homophones\n",
    "df_combo_003_with_binary['hasAltForm'] = df_combo_003_with_binary['Term2' and 'AlternateForms']!= \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 36. Drop non-numerical columns from combo data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_004_less_cols = df_combo_003_with_binary.copy()\n",
    "df_combo_004_less_cols = df_combo_004_less_cols.drop(['Examples','ExamplesAudio',\n",
    "                            'Meaning1','RelatedWords','Breakdown1','Comparison',\n",
    "                           'Usage','Prompt1','Prompt2','KakuMCD','IuMCD','ExtraMemo',\n",
    "                           'Breakdown2','Picture2','Picture3','Picture4','Mnemonic',\n",
    "                            'Yomi3','gChap','gBook','semester','gNumber','ArabicNumeral',\n",
    "                            'CounterKanji','SoloLookCards','HinshiMarker','Hint',\n",
    "                            'mod','Transliteration'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# casts columns of type object to type int as directed, use with caution\n",
    "def intify_bools(df, col):\n",
    "    df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 37. Ensure numerical/boolean types are encoded properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_004_less_cols.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in intify_list:\n",
    "    intify_bools(df_combo_004_less_cols,item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_004_less_cols.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 38. Further reduce columns not in use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_004_less_cols = df_combo_004_less_cols.drop(['Picture1','Sound','Sound2','Sound3','AtoQ','AtoQaudio',\n",
    "                              'AtoQkana','AtoQtranslation','QandApicture','answerPicture',\n",
    "                              'TagOverflow','Translation2','blank1','blank2',\n",
    "                              'Meaning2','Yomi2','Term2','SameSoundWords','hasPOS',\n",
    "                             'SimilarWords','AlternateForms','Translation3'],axis=1)\n",
    "\n",
    "df_combo_004_less_cols.head(35)[30:]\n",
    "\n",
    "#selection2 = df_binary.loc[df_binary['hasMultiMeaning']==1]\n",
    "#selection2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 39. Count syllable count & character length for each term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combo_005_with_len = df_combo_004_less_cols.copy()\n",
    "\n",
    "df_combo_005_with_len['TermLen'] = df_combo_005_with_len['Term'].str.len()\n",
    "df_combo_005_with_len['Syllables'] = df_combo_005_with_len['Yomi1'].str.len()\n",
    "df_combo_005_with_len.loc[df_combo_005_with_len['Syllables'] == 0, 'Syllables'] = df_combo_005_with_len['TermLen']\n",
    "\n",
    "bins = [0,1,2,4,8,128]\n",
    "labels = [\"[1]\",\"[2]\",\"[3:4]\",\"[5:8]\",\"[9: ]\"]\n",
    "# https://stackoverflow.com/questions/45273731/binning-column-with-python-pandas\n",
    "df_combo_005_with_len['TermLenGroup'] = pd.cut(df_combo_005_with_len['TermLen'], bins=bins, labels=labels)\n",
    "\n",
    "#df.loc[df['Grades'] <= 77, 'Grades'] = 100\n",
    "# https://stackoverflow.com/questions/42815768/pandas-adding-column-with-the-length-of-other-column-as-value\n",
    "#df_binary2.head(35)[30:]\n",
    "df_combo_005_with_len.tail(20)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the many syllable entries\n",
    "df_many_syl = df_combo_005_with_len.copy()\n",
    "many_syl = df_many_syl['Syllables'] > 20\n",
    "df_many_syl.loc[many_syl]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Information\n",
    "\n",
    "The Spaced Repetition Software (\\\"SRS\\\") used for the study of Japanese by student \\\"A\\\" is an open souce program called Anki. The algorithm used by it to \\\"graduate\\\" (also refered to as \\\"maturing\\\") study items (called cards) so that subsequent reviews/practices will be spaced into the future is referred to as SM-2. [Please click here for more information on the SM-2 algorithm used in Anki.](\"https://apps.ankiweb.net/docs/manual.html#what-algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression: classification/categorization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
