{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Analysis\n",
    "## Section 3: Analyze the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import notes\n",
    "\n",
    "\n",
    "# import cards\n",
    "\n",
    "\n",
    "# import combo\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observe Metadata (tag) Frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freq = pd.Series(' '.join(df_notes_final.tags).split()).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tag_freq.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Observations\n",
    "\n",
    "Looks like our data is ready for some proper inspection! What are some questions that we might ask of this dataset? We could start with some simple/basic broad/overview observations about the (condensed) dataset such as:\n",
    "- How many terms (unique notes) exist?\n",
    "- How many study vectors (unique card types) exist (were utilized by student A)?\n",
    "- When did student A first start studying?\n",
    "- What is the data distribution for reps count? For laspes count?\n",
    "- Of the terms that exist, how many had audio data?\n",
    "- Of the terms that exist, how many had image data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique terms in the condensed dataset\n",
    "len(df_new_cols['Term'].unique())\n",
    "\n",
    "# confirm what card types exist\n",
    "df_new_cols['ord'].value_counts()\n",
    "\n",
    "pd_crt # datetime of collection creation (studying commenced from this date)\n",
    "\n",
    "print(df_new_cols.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intitial Analysis\n",
    "\n",
    "There appears to be a linear relationship between lapses & reps. This seems to make sense and is worth keeping in mind (that lapses would, it seems, incur a cost of increasing reps). However, this info doesn't seem (to the author) directly actionable, whether it be simple correlation or even causation. The primary focus is what can be done to optimize studying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topical Analysis\n",
    "\n",
    "After doing some basic assessments of the data, we can dig a bit deeper:\n",
    "- Is there a correlation between words having multiple readings (\"yomi\") and their forget rate\\*?\n",
    "- Is there a correlation between words having same/similar sounding words and their forget rate\\*?\n",
    "- What might the effect of word length be on memorability? \\*\\*, \\*\\*\\*\n",
    "\n",
    "> \\* Forget rate can be understand as a multitude of things, such as the ratio between lapses & reps, as well as the raw lapse count, the average interval, and other numbers/ratios to be determined. I will attempt to clarify this in the process.  \n",
    "\\*\\* Memorability being loosely correlated with forget rate, where memorability could be understood as a word/term's intrinsic \"stickiness\" in the brain, as opposed to an individual or collective's capacity to keep words/terms in their head. Sources pending.  \n",
    "\\*\\*\\* A huge caveat here being that, this dataset has a sample size of 1 (for both student and language), so all observations, interpretations, and understandings must be taken with more than a few grains of salt (and tested further with larger sample sizes, of at least 200 students, and 5 or more languages)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "df_worked = df_binary2.copy() # 'ivl','factor','lapses'\n",
    "graph_drop_cols_1 = [\n",
    "    'nid','commonword','clothing','animal','body','food','textbook','college','place',\n",
    "    'fromdict','fromexam','onechar','n1','n2','n3','n4','n5','hasVisual',\n",
    "    'hasAudio','hasSimilar','hasAltForm','TermLen','Syllables','ivl_q']\n",
    "df_worked = df_worked.drop(graph_drop_cols_1,axis=1)\n",
    "corr = df_worked.corr()\n",
    "# https://stackoverflow.com/questions/38913965/make-the-size-of-a-heatmap-bigger-with-seaborn\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(corr, vmin=-1, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo: move this up one cell at least, if not up to the very top of initial graphing\n",
    "# Basic correlogram\n",
    "sns.pairplot(df_explore)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis\n",
    "\n",
    "For a deeper understanding of what it means to aquire new terminology, the researcher believes it best to conduct analysis on term acquisition by merging multiple vectors (individual cards) of a single term into single entries, where dummy values for each vector (such as review count, lapse count, etc.) are encoded per entry. This would enable inspection and correlation analysis of:\n",
    "- total reviews per term\n",
    "- average ratio of reviews per term per vector (look vs hear vs recall vs read)\n",
    "- where lapses are most likely to occur (per word, per vector, etc.)\n",
    "- how word length, presence of kanji, katakana, hirgana, or combination thereof, may affect the above counts & ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Information\n",
    "\n",
    "The Spaced Repetition Software (\\\"SRS\\\") used for the study of Japanese by student \\\"A\\\" is an open souce program called Anki. The algorithm used by it to \\\"graduate\\\" (also refered to as \\\"maturing\\\") study items (called cards) so that subsequent reviews/practices will be spaced into the future is referred to as SM-2. [Please click here for more information on the SM-2 algorithm used in Anki.](\"https://apps.ankiweb.net/docs/manual.html#what-algorithm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
