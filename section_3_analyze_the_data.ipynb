{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vocab Analysis\n",
    "## Section 3: Analyze the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "graphs_on = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load da_vocab_utility.py\n",
    "# TDD backbone assertion to confirm a function call returns the desired result\n",
    "def assertEquals(actual, expected, desc):\n",
    "    assert(actual==expected), desc + \" result: \" + str(actual) + \", expected: \" + str(expected)\n",
    "    return \"OK\"\n",
    "\t\n",
    "# check that two lists have the same contents\n",
    "def lists_equal(a,b):\n",
    "    return (a == b).all()\n",
    "\t\n",
    "\n",
    "# shallow check (by row) for duplicates\n",
    "def has_dupes(df_in):\n",
    "    dupe = df_in.duplicated()\n",
    "    return df_in.loc[dupe].shape[0] != 0\n",
    "\t\n",
    "def print_line_break():\n",
    "    print(\"-\"*75)\n",
    "\t\n",
    "def print_before_after(b, a, t=\"\"):\n",
    "    if t != \"\":\n",
    "        print_line_break()\n",
    "        print(t)\n",
    "    print_line_break()\n",
    "    print(\"Before: \" + str(b))\n",
    "    print_line_break()\n",
    "    print(\"After: \" + str(a))\n",
    "    print_line_break()\n",
    "\t\n",
    "def time_it(func, *args, **kwargs):\n",
    "    start = time.time()\n",
    "    func(*args, **kwargs)\n",
    "    end = time.time()\n",
    "    # https://stackoverflow.com/questions/8885663/how-to-format-a-floating-number-to-fixed-width-in-python\n",
    "    print(\"{:.0f}\".format((end - start)*1000) + \" miliseconds\")\n",
    "\t\n",
    "def has_dupe_terms(df_in):\n",
    "    location = df_in['Term'].duplicated()\n",
    "    return df_in.loc[location].shape[0] != 0\n",
    "\t\n",
    "def get_rows_by_value_in_col(df_in, value, col):\n",
    "    return df_in.loc[df_in[col]==value]\n",
    "\t\n",
    "# Converts a tag string to a list to a set back to a string (this removes the duplicates)\n",
    "def remove_dupes(t):\n",
    "    temp = list(set(t.lower().split()))\n",
    "    return ' '.join(temp) # return as string\n",
    "\t\n",
    "# determines if an individual tag substring exists in a larger tags list string\n",
    "def tag_exists(tags, tag):\n",
    "    return 1 if tag in tags.split() else 0\n",
    "\t\n",
    "def is_blank (s):\n",
    "    return not (s and s.strip())\n",
    "\t\n",
    "def get_frame_of_cards_by_term(df, t):\n",
    "    return df.loc[df['Term']==t]\n",
    "\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show all columns\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_fig_a(x):\n",
    "    plt.savefig(\"analyze_\" + str(x)+\".png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Import necessary datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/36977223/how-should-i-read-a-csv-file-without-the-unnamed-row-with-pandas?noredirect=1&lq=1\n",
    "# import notes\n",
    "notes_location = \"datasets/df_notes_020_final_section_2.csv\"\n",
    "df_notes = pd.read_csv(notes_location, index_col=[0])\n",
    "\n",
    "# import cards\n",
    "cards_location = \"datasets/df_cards_012_mid_section_2.csv\"\n",
    "df_cards = pd.read_csv(cards_location, index_col=[0])\n",
    "\n",
    "# todo: import revlog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>NoteCreated</th>\n",
       "      <th>LastModified</th>\n",
       "      <th>commonword</th>\n",
       "      <th>clothing</th>\n",
       "      <th>animal</th>\n",
       "      <th>body</th>\n",
       "      <th>food</th>\n",
       "      <th>place</th>\n",
       "      <th>textbook</th>\n",
       "      <th>college</th>\n",
       "      <th>fromdict</th>\n",
       "      <th>fromexam</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>katakana</th>\n",
       "      <th>hiragana</th>\n",
       "      <th>kanji</th>\n",
       "      <th>adv</th>\n",
       "      <th>adj</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>nonconvo</th>\n",
       "      <th>convo</th>\n",
       "      <th>metalite</th>\n",
       "      <th>hasSimilarSound</th>\n",
       "      <th>hasSameSound</th>\n",
       "      <th>hasVisual</th>\n",
       "      <th>hasAudio</th>\n",
       "      <th>hasMultiMeaning</th>\n",
       "      <th>hasMultiReading</th>\n",
       "      <th>hasSimilarMeaning</th>\n",
       "      <th>hasAltForm</th>\n",
       "      <th>hasRichExamples</th>\n",
       "      <th>TermLen</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>TermLenGroup</th>\n",
       "      <th>SyllablesGroup</th>\n",
       "      <th>jlpt_lvl_d</th>\n",
       "      <th>script</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "      <th>mean_ivl</th>\n",
       "      <th>mean_factor</th>\n",
       "      <th>mean_reps</th>\n",
       "      <th>mean_lapses</th>\n",
       "      <th>c_suff_reviewed_x</th>\n",
       "      <th>total_reps</th>\n",
       "      <th>total_lapses</th>\n",
       "      <th>hasListenCard</th>\n",
       "      <th>hasPictureCard</th>\n",
       "      <th>hasReadCard</th>\n",
       "      <th>hasTranslateCard</th>\n",
       "      <th>c_suff_reviewed_y</th>\n",
       "      <th>mean_note_waste</th>\n",
       "      <th>mean_note_roi</th>\n",
       "      <th>n_ivl_q</th>\n",
       "      <th>n_factor_q</th>\n",
       "      <th>n_waste_q</th>\n",
       "      <th>n_roi_q</th>\n",
       "      <th>no_waste</th>\n",
       "      <th>analysis_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1331799797114</td>\n",
       "      <td>commonword noun kanji suruverb fromdict</td>\n",
       "      <td>移籍</td>\n",
       "      <td>いせき</td>\n",
       "      <td>2012-03-15 08:23:17.114</td>\n",
       "      <td>2019-06-09 23:34:05.000</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3:4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanji</td>\n",
       "      <td>1.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.142857</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1331799797126</td>\n",
       "      <td>kanji fromdict</td>\n",
       "      <td>有能</td>\n",
       "      <td>ゆうのう</td>\n",
       "      <td>2012-03-15 08:23:17.126</td>\n",
       "      <td>2019-05-27 20:00:11.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>[2]</td>\n",
       "      <td>[3:4]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanji</td>\n",
       "      <td>1.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>2130.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27.555556</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1331799797127</td>\n",
       "      <td>transportation noun travel mixedscript haskanj...</td>\n",
       "      <td>公衆トイレ</td>\n",
       "      <td>こうしゅうトイレ</td>\n",
       "      <td>2012-03-15 08:23:17.127</td>\n",
       "      <td>2019-05-27 20:59:39.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>[5:8]</td>\n",
       "      <td>[5:8]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>229.0</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25.444444</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1331799797128</td>\n",
       "      <td>kanji fromdict</td>\n",
       "      <td>送り賃</td>\n",
       "      <td>おくりちん</td>\n",
       "      <td>2012-03-15 08:23:17.128</td>\n",
       "      <td>2019-05-18 12:54:16.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>[3:4]</td>\n",
       "      <td>[5:8]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanji</td>\n",
       "      <td>1.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>2120.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>22.250000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1331799797130</td>\n",
       "      <td>technical kanji fromdict noun</td>\n",
       "      <td>量子物理学</td>\n",
       "      <td>りょうしぶつりがく</td>\n",
       "      <td>2012-03-15 08:23:17.130</td>\n",
       "      <td>2019-05-28 00:40:16.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>[5:8]</td>\n",
       "      <td>[9: ]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kanji</td>\n",
       "      <td>1.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>2270.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>29.142857</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             nid                                               tags   Term  \\\n",
       "0  1331799797114            commonword noun kanji suruverb fromdict     移籍   \n",
       "4  1331799797126                                     kanji fromdict     有能   \n",
       "5  1331799797127  transportation noun travel mixedscript haskanj...  公衆トイレ   \n",
       "6  1331799797128                                     kanji fromdict    送り賃   \n",
       "7  1331799797130                      technical kanji fromdict noun  量子物理学   \n",
       "\n",
       "       Yomi1              NoteCreated             LastModified  commonword  \\\n",
       "0        いせき  2012-03-15 08:23:17.114  2019-06-09 23:34:05.000           1   \n",
       "4       ゆうのう  2012-03-15 08:23:17.126  2019-05-27 20:00:11.000           0   \n",
       "5   こうしゅうトイレ  2012-03-15 08:23:17.127  2019-05-27 20:59:39.000           0   \n",
       "6      おくりちん  2012-03-15 08:23:17.128  2019-05-18 12:54:16.000           0   \n",
       "7  りょうしぶつりがく  2012-03-15 08:23:17.130  2019-05-28 00:40:16.000           0   \n",
       "\n",
       "   clothing  animal  body  food  place  textbook  college  fromdict  fromexam  \\\n",
       "0         0       0     0     0      0         0        0         1         0   \n",
       "4         0       0     0     0      0         0        0         1         0   \n",
       "5         0       0     0     0      0         0        0         0         0   \n",
       "6         0       0     0     0      0         0        0         1         0   \n",
       "7         0       0     0     0      0         0        0         1         0   \n",
       "\n",
       "   n1  n2  n3  n4  n5  katakana  hiragana  kanji  adv  adj  noun  verb  \\\n",
       "0   0   0   0   0   0         0         0      1    0    0     1     0   \n",
       "4   0   0   0   0   0         0         0      1    0    0     0     0   \n",
       "5   0   0   0   0   0         0         0      0    0    0     1     0   \n",
       "6   0   0   0   0   0         0         0      1    0    0     0     0   \n",
       "7   0   0   0   0   0         0         0      1    0    0     1     0   \n",
       "\n",
       "   nonconvo  convo  metalite  hasSimilarSound  hasSameSound  hasVisual  \\\n",
       "0         0      0         0                0             0          0   \n",
       "4         0      0         0                0             0          0   \n",
       "5         0      0         0                0             0          1   \n",
       "6         0      0         0                0             0          0   \n",
       "7         0      0         0                0             0          0   \n",
       "\n",
       "   hasAudio  hasMultiMeaning  hasMultiReading  hasSimilarMeaning  hasAltForm  \\\n",
       "0         0                0                0                  0           0   \n",
       "4         0                0                0                  1           0   \n",
       "5         0                0                0                  0           0   \n",
       "6         0                0                0                  0           0   \n",
       "7         0                0                0                  0           0   \n",
       "\n",
       "   hasRichExamples  TermLen  Syllables TermLenGroup SyllablesGroup  \\\n",
       "0                0        2          3          [2]          [3:4]   \n",
       "4                0        2          4          [2]          [3:4]   \n",
       "5                0        5          8        [5:8]          [5:8]   \n",
       "6                0        3          5        [3:4]          [5:8]   \n",
       "7                0        5          9        [5:8]          [9: ]   \n",
       "\n",
       "   jlpt_lvl_d script  c_suff_reviewed  mean_ivl  mean_factor  mean_reps  \\\n",
       "0         NaN  kanji              1.0      99.0       1980.0        7.0   \n",
       "4         NaN  kanji              1.0     248.0       2130.0        9.0   \n",
       "5         NaN    NaN              1.0     229.0       2270.0        9.0   \n",
       "6         NaN  kanji              1.0     178.0       2120.0        8.0   \n",
       "7         NaN  kanji              1.0     204.0       2270.0        7.0   \n",
       "\n",
       "   mean_lapses  c_suff_reviewed_x  total_reps  total_lapses  hasListenCard  \\\n",
       "0          0.0                1.0           7             0              0   \n",
       "4          0.0                1.0           9             0              0   \n",
       "5          0.0                1.0           9             0              0   \n",
       "6          0.0                1.0           8             0              0   \n",
       "7          0.0                1.0           7             0              0   \n",
       "\n",
       "   hasPictureCard  hasReadCard  hasTranslateCard  c_suff_reviewed_y  \\\n",
       "0               0            1                 0                1.0   \n",
       "4               0            1                 0                1.0   \n",
       "5               0            1                 0                1.0   \n",
       "6               0            1                 0                1.0   \n",
       "7               0            1                 0                1.0   \n",
       "\n",
       "   mean_note_waste  mean_note_roi  n_ivl_q  n_factor_q  n_waste_q  n_roi_q  \\\n",
       "0              0.0      14.142857        0           2          0        1   \n",
       "4              0.0      27.555556        2           2          0        3   \n",
       "5              0.0      25.444444        2           2          0        2   \n",
       "6              0.0      22.250000        1           2          0        2   \n",
       "7              0.0      29.142857        1           2          0        3   \n",
       "\n",
       "   no_waste analysis_cat  \n",
       "0         1          NaN  \n",
       "4         1          NaN  \n",
       "5         1          NaN  \n",
       "6         1          NaN  \n",
       "7         1          NaN  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[2]' '[5:8]' '[3:4]' '[1]' '[9: ]']\n",
      "['[3:4]' '[5:8]' '[9: ]' '[2]' '[1]']\n",
      "['kanji' nan 'katakana' 'hiragana']\n",
      "[nan 'sticky' 'slippery']\n"
     ]
    }
   ],
   "source": [
    "print(df_notes.TermLenGroup.unique())\n",
    "print(df_notes.SyllablesGroup.unique())\n",
    "print(df_notes.script.unique())\n",
    "print(df_notes.analysis_cat.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>nid</th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "      <th>CardCreated</th>\n",
       "      <th>DueDate</th>\n",
       "      <th>c_ivl_q</th>\n",
       "      <th>c_factor_q</th>\n",
       "      <th>CardType_listen</th>\n",
       "      <th>CardType_look</th>\n",
       "      <th>CardType_read</th>\n",
       "      <th>CardType_recall</th>\n",
       "      <th>cardtype</th>\n",
       "      <th>waste</th>\n",
       "      <th>roi</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1331799797114</td>\n",
       "      <td>1331799797114</td>\n",
       "      <td>99</td>\n",
       "      <td>1980</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-15 08:23:17.114</td>\n",
       "      <td>2015-02-04 09:00:00.000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>read</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>14.142857</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1331799797122</td>\n",
       "      <td>1331799797122</td>\n",
       "      <td>224</td>\n",
       "      <td>2130</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-15 08:23:17.122</td>\n",
       "      <td>2015-07-04 09:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>read</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>44.800000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1331799797125</td>\n",
       "      <td>1331799797125</td>\n",
       "      <td>291</td>\n",
       "      <td>1930</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2012-03-15 08:23:17.125</td>\n",
       "      <td>2016-01-30 09:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>read</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>16.166667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1331799797126</td>\n",
       "      <td>1331799797126</td>\n",
       "      <td>248</td>\n",
       "      <td>2130</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-15 08:23:17.126</td>\n",
       "      <td>2015-09-04 09:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>read</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>27.555556</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1331799797127</td>\n",
       "      <td>1331799797127</td>\n",
       "      <td>229</td>\n",
       "      <td>2270</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2012-03-15 08:23:17.127</td>\n",
       "      <td>2015-06-11 09:00:00.000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>read</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>25.444444</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              cid            nid  ivl  factor  reps  lapses  \\\n",
       "3   1331799797114  1331799797114   99    1980     7       0   \n",
       "8   1331799797122  1331799797122  224    2130     5       0   \n",
       "9   1331799797125  1331799797125  291    1930    18       1   \n",
       "10  1331799797126  1331799797126  248    2130     9       0   \n",
       "11  1331799797127  1331799797127  229    2270     9       0   \n",
       "\n",
       "                CardCreated                  DueDate  c_ivl_q  c_factor_q  \\\n",
       "3   2012-03-15 08:23:17.114  2015-02-04 09:00:00.000        0           1   \n",
       "8   2012-03-15 08:23:17.122  2015-07-04 09:00:00.000        2           2   \n",
       "9   2012-03-15 08:23:17.125  2016-01-30 09:00:00.000        2           1   \n",
       "10  2012-03-15 08:23:17.126  2015-09-04 09:00:00.000        2           2   \n",
       "11  2012-03-15 08:23:17.127  2015-06-11 09:00:00.000        2           2   \n",
       "\n",
       "    CardType_listen  CardType_look  CardType_read  CardType_recall cardtype  \\\n",
       "3                 0              0              1                0     read   \n",
       "8                 0              0              1                0     read   \n",
       "9                 0              0              1                0     read   \n",
       "10                0              0              1                0     read   \n",
       "11                0              0              1                0     read   \n",
       "\n",
       "       waste        roi  c_suff_reviewed  \n",
       "3   0.142857  14.142857              1.0  \n",
       "8   0.200000  44.800000              1.0  \n",
       "9   0.111111  16.166667              1.0  \n",
       "10  0.111111  27.555556              1.0  \n",
       "11  0.111111  25.444444              1.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Observe Metadata (tag) Frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_freq = pd.Series(' '.join(df_notes.tags).split()).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "kanji                   3932\n",
       "textbook                1662\n",
       "fromdict                1228\n",
       "metalite                1042\n",
       "verb                     837\n",
       "fromtest                 836\n",
       "commonword               488\n",
       "noun                     402\n",
       "hasrobo                  313\n",
       "fromexam                 305\n",
       "media                    297\n",
       "hiragana                 260\n",
       "checked                  220\n",
       "n3                       207\n",
       "addsimilar               196\n",
       "numeric                  193\n",
       "usuallywritteninkana     188\n",
       "katakana                 187\n",
       "convo                    153\n",
       "transitive               147\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "college         146\n",
       "music           133\n",
       "lyrics          122\n",
       "intransitive    107\n",
       "iadj            106\n",
       "adj             106\n",
       "place           106\n",
       "n2              106\n",
       "counter         102\n",
       "n4               99\n",
       "technical        85\n",
       "multimeaning     85\n",
       "n1               81\n",
       "hassame          79\n",
       "n5               78\n",
       "semester1        75\n",
       "noadjective      66\n",
       "multireading     61\n",
       "suruverb         56\n",
       "hassimilar       53\n",
       "body             52\n",
       "animal           52\n",
       "name             49\n",
       "geography        49\n",
       "culture          49\n",
       "type5r           48\n",
       "food             47\n",
       "haskanji         46\n",
       "adv              44\n",
       "japan1st         37\n",
       "people           36\n",
       "gairaigo         35\n",
       "multiwriting     33\n",
       "seenbynative     32\n",
       "onomatopoeic     31\n",
       "ghibli           30\n",
       "multiterm        29\n",
       "magazine         29\n",
       "suffix           27\n",
       "abbr             27\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_freq.head(60)[20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cooking              1\n",
       "hazard               1\n",
       "nojishoentry         1\n",
       "frommagazine         1\n",
       "multiform            1\n",
       "dialect              1\n",
       "checksimilar         1\n",
       "bathroom             1\n",
       "verbscompoundpast    1\n",
       "toy                  1\n",
       "nounsuffix           1\n",
       "tradition            1\n",
       "checkhint            1\n",
       "hospital             1\n",
       "category             1\n",
       "familiar             1\n",
       "challenging          1\n",
       "seafood              1\n",
       "statistics           1\n",
       "grammarcontext       1\n",
       "dailylife            1\n",
       "casual               1\n",
       "fish                 1\n",
       "common               1\n",
       "emergency            1\n",
       "notinjapan           1\n",
       "space                1\n",
       "vivid                1\n",
       "hassamemeaning       1\n",
       "position             1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect tags that have been used only sparingly\n",
    "tag_freq.tail(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Observations\n",
    "\n",
    "Looks like our data is ready for some proper inspection! What are some questions that we might ask of this dataset? We could start with some simple/basic broad/overview observations about the (condensed) dataset such as:\n",
    "- How many terms (unique notes) exist?\n",
    "- How many study vectors (unique card types) exist (were utilized by student A)?\n",
    "- When did student A first start studying?\n",
    "- What is the data distribution for reps count? For laspes count?\n",
    "- Of the terms that exist, how many had audio data?\n",
    "- Of the terms that exist, how many had image data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5471"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unique terms in the condensed dataset\n",
    "len(df_notes['Term'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "read      6304\n",
       "look       674\n",
       "listen      39\n",
       "recall       7\n",
       "Name: cardtype, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# confirm what card types exist\n",
    "df_cards['cardtype'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd_crt # datetime of collection creation (studying commenced from this date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7024, 18)\n"
     ]
    }
   ],
   "source": [
    "print(df_cards.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5471, 67)\n"
     ]
    }
   ],
   "source": [
    "print(df_notes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#components of binary list (cards)\n",
    "type_list = ['CardType_listen','CardType_look','CardType_read','CardType_recall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#components of binary list (notes, combo)\n",
    "genre_list = ['clothing','animal','body','food','place']\n",
    "\n",
    "source_list = ['fromdict','fromexam','textbook','college']\n",
    "\n",
    "convo_list = ['convo','nonconvo']\n",
    "\n",
    "jlpt_list = ['n1','n2','n3','n4','n5']\n",
    "\n",
    "pos_list = ['noun','verb','adj','adv']\n",
    "\n",
    "char_list = ['katakana','hiragana','kanji']\n",
    "\n",
    "has_list = ['hasVisual','hasAudio','hasMultiMeaning','hasMultiReading','hasSimilarSound','hasSameSound',\n",
    "    'hasSimilarMeaning','hasAltForm','hasRichExamples']\n",
    "\n",
    "card_list = ['hasListenCard','hasPictureCard','hasReadCard','hasTranslateCard']\n",
    "\n",
    "other_list = ['commonword','metalite']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#components of continuous list\n",
    "len_list = ['TermLen','Syllables']\n",
    "\n",
    "study_data_list = ['mean_ivl','mean_factor','mean_reps','mean_lapses',\n",
    "                   'total_reps','total_lapses']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_list = list(genre_list + source_list + jlpt_list +\n",
    "    pos_list + char_list + has_list + other_list + convo_list) #card_list\n",
    "\n",
    "continuous_list = list(len_list + study_data_list)\n",
    "\n",
    "discrete_non_binary_list = ['NoteCreated','LastModified','TermLenGroup','SyllablesGroup','jlpt_lvl_d']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_note_fields = ['mean_ivl','mean_factor','mean_reps','mean_lapses',\n",
    "                       'total_reps','total_lapses']\n",
    "\n",
    "mean_card_fields = ['mean_card_waste','mean_card_roi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "note_quintiles = ['n_ivl_q','n_factor_q','n_waste_q','n_roi_q']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "df_cards_001_corr = df_cards.copy()\n",
    "df_cards_001_corr = df_cards_001_corr.drop([\"cid\",\"nid\",'c_ivl_q','c_factor_q','ivl','factor','reps','lapses'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect card correlations visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if graphs_on:\n",
    "    corr_card = df_cards_001_corr.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax_card = sns.heatmap(corr_card, vmin=-1, cmap=\"YlGnBu\", annot=True)\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect note correlations visually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "df_notes_001_corr = df_notes.copy()\n",
    "df_notes_001_corr = df_notes_001_corr.drop(list(\n",
    "    [\"nid\",'jlpt_lvl_d','no_waste'] +\n",
    "    card_list + convo_list + numeric_note_fields + genre_list + \n",
    "    source_list + jlpt_list + char_list + has_list + \n",
    "    other_list + len_list + note_quintiles), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the correlation between word type & mean note ROI & mean note waste\n",
    "if graphs_on:\n",
    "    corr = df_notes_001_corr.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax = sns.heatmap(corr, vmin=-1, cmap=\"YlGnBu\", annot=True)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_002_corr = df_notes.copy()\n",
    "df_notes_002_corr = df_notes_002_corr.drop(list(\n",
    "    [\"nid\",\"total_reps\",\"total_lapses\",'no_waste'] + \n",
    "    numeric_note_fields + source_list + convo_list + pos_list + len_list + other_list + \n",
    "    has_list + genre_list + jlpt_list + card_list + note_quintiles + ['jlpt_lvl_d','mean_factor']), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect correlations by script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "if graphs_on:\n",
    "    corr2 = df_notes_002_corr.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax2 = sns.heatmap(corr2, vmin=-1, cmap=\"YlGnBu\", annot=True)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_003_corr = df_notes.copy()\n",
    "df_notes_003_corr = df_notes_003_corr.drop(list(\n",
    "    [\"nid\",\"total_reps\",\"total_lapses\",'jlpt_lvl_d','mean_factor','no_waste'] + \n",
    "    numeric_note_fields + convo_list + char_list + pos_list + len_list + other_list + \n",
    "    has_list + genre_list + jlpt_list + card_list + note_quintiles), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect correlations by word first encounter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "if graphs_on:\n",
    "    corr3 = df_notes_003_corr.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax3 = sns.heatmap(corr3, vmin=-1, cmap=\"YlGnBu\", annot=True)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_004_corr = df_notes.copy()\n",
    "df_notes_004_corr = df_notes_004_corr.drop(list(\n",
    "    [\"nid\",\"total_reps\",\"total_lapses\",'jlpt_lvl_d','mean_factor',\n",
    "     'commonword','no_waste'] + numeric_note_fields + char_list + source_list + \n",
    "    pos_list + len_list + has_list + genre_list + jlpt_list + card_list + note_quintiles), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect correlation of convo ok'd vs not, & metadata poorness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "if graphs_on:\n",
    "    corr4 = df_notes_004_corr.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax4 = sns.heatmap(corr4, vmin=-1, cmap=\"YlGnBu\", annot=True)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inspect field data correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_005_corr = df_notes.copy()\n",
    "df_notes_005_corr = df_notes_005_corr.drop(list(\n",
    "    [\"nid\",\"total_reps\",\"total_lapses\",'jlpt_lvl_d','mean_factor',\n",
    "     'metalite','no_waste']+other_list+convo_list+char_list+numeric_note_fields+\n",
    "    source_list+pos_list+len_list+genre_list+jlpt_list+card_list + note_quintiles), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show correlation of stats via heatmap\n",
    "if graphs_on:\n",
    "    corr5 = df_notes_005_corr.corr()\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax5 = sns.heatmap(corr5, vmin=-1, cmap=\"YlGnBu\", annot=True)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "      <th>waste</th>\n",
       "      <th>roi</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7024.000000</td>\n",
       "      <td>7024.000000</td>\n",
       "      <td>7024.000000</td>\n",
       "      <td>7024.000000</td>\n",
       "      <td>7024.000000</td>\n",
       "      <td>7024.000000</td>\n",
       "      <td>7024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>328.458144</td>\n",
       "      <td>1712.236902</td>\n",
       "      <td>15.954869</td>\n",
       "      <td>0.653189</td>\n",
       "      <td>0.106348</td>\n",
       "      <td>27.550559</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>286.170879</td>\n",
       "      <td>386.865972</td>\n",
       "      <td>9.231241</td>\n",
       "      <td>1.218840</td>\n",
       "      <td>0.035868</td>\n",
       "      <td>25.132115</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033333</td>\n",
       "      <td>0.008850</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>162.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>9.454545</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>235.000000</td>\n",
       "      <td>1639.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>19.230769</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>402.000000</td>\n",
       "      <td>2050.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>38.511364</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2148.000000</td>\n",
       "      <td>2710.000000</td>\n",
       "      <td>113.000000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.210526</td>\n",
       "      <td>116.090909</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               ivl       factor         reps       lapses        waste  \\\n",
       "count  7024.000000  7024.000000  7024.000000  7024.000000  7024.000000   \n",
       "mean    328.458144  1712.236902    15.954869     0.653189     0.106348   \n",
       "std     286.170879   386.865972     9.231241     1.218840     0.035868   \n",
       "min       1.000000  1300.000000     5.000000     0.000000     0.033333   \n",
       "25%     162.000000  1300.000000    10.000000     0.000000     0.076923   \n",
       "50%     235.000000  1639.500000    14.000000     0.000000     0.100000   \n",
       "75%     402.000000  2050.000000    19.000000     1.000000     0.125000   \n",
       "max    2148.000000  2710.000000   113.000000    16.000000     0.210526   \n",
       "\n",
       "               roi  c_suff_reviewed  \n",
       "count  7024.000000           7024.0  \n",
       "mean     27.550559              1.0  \n",
       "std      25.132115              0.0  \n",
       "min       0.008850              1.0  \n",
       "25%       9.454545              1.0  \n",
       "50%      19.230769              1.0  \n",
       "75%      38.511364              1.0  \n",
       "max     116.090909              1.0  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards_002_describe = df_cards.copy()\n",
    "df_cards_002_describe = df_cards_002_describe.drop(list([\"cid\",\"nid\",'c_ivl_q','c_factor_q']+type_list), axis=1)\n",
    "\n",
    "df_cards_002_describe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "      <th>waste</th>\n",
       "      <th>roi</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ivl</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.232198</td>\n",
       "      <td>-0.172093</td>\n",
       "      <td>-0.215334</td>\n",
       "      <td>-0.129463</td>\n",
       "      <td>0.820618</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>factor</th>\n",
       "      <td>0.232198</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.651075</td>\n",
       "      <td>-0.404378</td>\n",
       "      <td>0.465315</td>\n",
       "      <td>0.587608</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reps</th>\n",
       "      <td>-0.172093</td>\n",
       "      <td>-0.651075</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.862316</td>\n",
       "      <td>-0.131623</td>\n",
       "      <td>-0.478978</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lapses</th>\n",
       "      <td>-0.215334</td>\n",
       "      <td>-0.404378</td>\n",
       "      <td>0.862316</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.325635</td>\n",
       "      <td>-0.389980</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waste</th>\n",
       "      <td>-0.129463</td>\n",
       "      <td>0.465315</td>\n",
       "      <td>-0.131623</td>\n",
       "      <td>0.325635</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.134688</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>roi</th>\n",
       "      <td>0.820618</td>\n",
       "      <td>0.587608</td>\n",
       "      <td>-0.478978</td>\n",
       "      <td>-0.389980</td>\n",
       "      <td>0.134688</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_suff_reviewed</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      ivl    factor      reps    lapses     waste       roi  \\\n",
       "ivl              1.000000  0.232198 -0.172093 -0.215334 -0.129463  0.820618   \n",
       "factor           0.232198  1.000000 -0.651075 -0.404378  0.465315  0.587608   \n",
       "reps            -0.172093 -0.651075  1.000000  0.862316 -0.131623 -0.478978   \n",
       "lapses          -0.215334 -0.404378  0.862316  1.000000  0.325635 -0.389980   \n",
       "waste           -0.129463  0.465315 -0.131623  0.325635  1.000000  0.134688   \n",
       "roi              0.820618  0.587608 -0.478978 -0.389980  0.134688  1.000000   \n",
       "c_suff_reviewed       NaN       NaN       NaN       NaN       NaN       NaN   \n",
       "\n",
       "                 c_suff_reviewed  \n",
       "ivl                          NaN  \n",
       "factor                       NaN  \n",
       "reps                         NaN  \n",
       "lapses                       NaN  \n",
       "waste                        NaN  \n",
       "roi                          NaN  \n",
       "c_suff_reviewed              NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards_002_describe.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROI trends for cards using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS is Ordinary Least Squares, the most common type of linear regression\n",
    "#the fit function uses the predictive values to calculate the best linear regression line\n",
    "result = smf.ols('roi ~ ivl + factor - 1', data=df_cards).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>roi</td>       <th>  R-squared:         </th> <td>   0.872</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.872</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>2.382e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 10 Jun 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:07:44</td>     <th>  Log-Likelihood:    </th> <td> -28177.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  7024</td>      <th>  AIC:               </th> <td>5.636e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  7022</td>      <th>  BIC:               </th> <td>5.637e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "     <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>ivl</th>    <td>    0.0626</td> <td>    0.001</td> <td>  109.369</td> <td> 0.000</td> <td>    0.062</td> <td>    0.064</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>factor</th> <td>    0.0052</td> <td>    0.000</td> <td>   36.621</td> <td> 0.000</td> <td>    0.005</td> <td>    0.005</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>2104.296</td> <th>  Durbin-Watson:     </th> <td>   1.688</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>8400.759</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.437</td>  <th>  Prob(JB):          </th> <td>    0.00</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 7.521</td>  <th>  Cond. No.          </th> <td>    6.54</td>\n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    roi   R-squared:                       0.872\n",
       "Model:                            OLS   Adj. R-squared:                  0.872\n",
       "Method:                 Least Squares   F-statistic:                 2.382e+04\n",
       "Date:                Mon, 10 Jun 2019   Prob (F-statistic):               0.00\n",
       "Time:                        02:07:44   Log-Likelihood:                -28177.\n",
       "No. Observations:                7024   AIC:                         5.636e+04\n",
       "Df Residuals:                    7022   BIC:                         5.637e+04\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "ivl            0.0626      0.001    109.369      0.000       0.062       0.064\n",
       "factor         0.0052      0.000     36.621      0.000       0.005       0.005\n",
       "==============================================================================\n",
       "Omnibus:                     2104.296   Durbin-Watson:                   1.688\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):             8400.759\n",
       "Skew:                           1.437   Prob(JB):                         0.00\n",
       "Kurtosis:                       7.521   Cond. No.                         6.54\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROI trends for notes using linear regression (using everything)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS is Ordinary Least Squares, the most common type of linear regression\n",
    "#the fit function uses the predictive values to calculate the best linear regression line\n",
    "result = smf.ols('mean_note_roi ~ Syllables + TermLen + mean_reps + total_reps - 1', data=df_notes).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>mean_note_roi</td>  <th>  R-squared:         </th> <td>   0.580</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.580</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1891.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 10 Jun 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:07:44</td>     <th>  Log-Likelihood:    </th> <td> -23683.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5471</td>      <th>  AIC:               </th> <td>4.737e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5467</td>      <th>  BIC:               </th> <td>4.740e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     4</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Syllables</th>  <td>    2.3382</td> <td>    0.188</td> <td>   12.407</td> <td> 0.000</td> <td>    1.969</td> <td>    2.708</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TermLen</th>    <td>    5.1002</td> <td>    0.282</td> <td>   18.072</td> <td> 0.000</td> <td>    4.547</td> <td>    5.653</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_reps</th>  <td>   -0.5940</td> <td>    0.050</td> <td>  -11.772</td> <td> 0.000</td> <td>   -0.693</td> <td>   -0.495</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>total_reps</th> <td>    0.3114</td> <td>    0.039</td> <td>    7.905</td> <td> 0.000</td> <td>    0.234</td> <td>    0.389</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>364.448</td> <th>  Durbin-Watson:     </th> <td>   1.721</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 623.368</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.509</td>  <th>  Prob(JB):          </th> <td>4.34e-136</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 4.303</td>  <th>  Cond. No.          </th> <td>    37.9</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          mean_note_roi   R-squared:                       0.580\n",
       "Model:                            OLS   Adj. R-squared:                  0.580\n",
       "Method:                 Least Squares   F-statistic:                     1891.\n",
       "Date:                Mon, 10 Jun 2019   Prob (F-statistic):               0.00\n",
       "Time:                        02:07:44   Log-Likelihood:                -23683.\n",
       "No. Observations:                5471   AIC:                         4.737e+04\n",
       "Df Residuals:                    5467   BIC:                         4.740e+04\n",
       "Df Model:                           4                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Syllables      2.3382      0.188     12.407      0.000       1.969       2.708\n",
       "TermLen        5.1002      0.282     18.072      0.000       4.547       5.653\n",
       "mean_reps     -0.5940      0.050    -11.772      0.000      -0.693      -0.495\n",
       "total_reps     0.3114      0.039      7.905      0.000       0.234       0.389\n",
       "==============================================================================\n",
       "Omnibus:                      364.448   Durbin-Watson:                   1.721\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              623.368\n",
       "Skew:                           0.509   Prob(JB):                    4.34e-136\n",
       "Kurtosis:                       4.303   Cond. No.                         37.9\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROI trends for notes using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS is Ordinary Least Squares, the most common type of linear regression\n",
    "#the fit function uses the predictive values to calculate the best linear regression line\n",
    "result = smf.ols('mean_note_roi ~ Syllables + TermLen - 1', data=df_notes).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>      <td>mean_note_roi</td>  <th>  R-squared:         </th> <td>   0.567</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.567</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3587.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 10 Jun 2019</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:07:44</td>     <th>  Log-Likelihood:    </th> <td> -23767.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5471</td>      <th>  AIC:               </th> <td>4.754e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5469</td>      <th>  BIC:               </th> <td>4.755e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Syllables</th> <td>    1.7058</td> <td>    0.181</td> <td>    9.403</td> <td> 0.000</td> <td>    1.350</td> <td>    2.061</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>TermLen</th>   <td>    4.8114</td> <td>    0.285</td> <td>   16.876</td> <td> 0.000</td> <td>    4.252</td> <td>    5.370</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>406.708</td> <th>  Durbin-Watson:     </th> <td>   1.735</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 517.321</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.678</td>  <th>  Prob(JB):          </th> <td>4.63e-113</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 3.657</td>  <th>  Cond. No.          </th> <td>    7.05</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:          mean_note_roi   R-squared:                       0.567\n",
       "Model:                            OLS   Adj. R-squared:                  0.567\n",
       "Method:                 Least Squares   F-statistic:                     3587.\n",
       "Date:                Mon, 10 Jun 2019   Prob (F-statistic):               0.00\n",
       "Time:                        02:07:44   Log-Likelihood:                -23767.\n",
       "No. Observations:                5471   AIC:                         4.754e+04\n",
       "Df Residuals:                    5469   BIC:                         4.755e+04\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Syllables      1.7058      0.181      9.403      0.000       1.350       2.061\n",
       "TermLen        4.8114      0.285     16.876      0.000       4.252       5.370\n",
       "==============================================================================\n",
       "Omnibus:                      406.708   Durbin-Watson:                   1.735\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              517.321\n",
       "Skew:                           0.678   Prob(JB):                    4.63e-113\n",
       "Kurtosis:                       3.657   Cond. No.                         7.05\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot waste trends for notes using linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS is Ordinary Least Squares, the most common type of linear regression\n",
    "#the fit function uses the predictive values to calculate the best linear regression line\n",
    "result = smf.ols('mean_note_waste ~ mean_ivl + mean_factor - 1', data=df_notes).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>     <td>mean_note_waste</td> <th>  R-squared:         </th>  <td>   0.281</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th>  <td>   0.281</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th>  <td>   1071.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 10 Jun 2019</td> <th>  Prob (F-statistic):</th>   <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>02:07:44</td>     <th>  Log-Likelihood:    </th>  <td>  10083.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>  5471</td>      <th>  AIC:               </th> <td>-2.016e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>  5469</td>      <th>  BIC:               </th> <td>-2.015e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>      <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>          <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_ivl</th>    <td>-2.492e-05</td> <td> 2.64e-06</td> <td>   -9.429</td> <td> 0.000</td> <td>-3.01e-05</td> <td>-1.97e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>mean_factor</th> <td> 1.809e-05</td> <td>  5.5e-07</td> <td>   32.881</td> <td> 0.000</td> <td>  1.7e-05</td> <td> 1.92e-05</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>609.617</td> <th>  Durbin-Watson:     </th> <td>   1.884</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td> 777.624</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.902</td>  <th>  Prob(JB):          </th> <td>1.38e-169</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.608</td>  <th>  Cond. No.          </th> <td>    8.96</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:        mean_note_waste   R-squared:                       0.281\n",
       "Model:                            OLS   Adj. R-squared:                  0.281\n",
       "Method:                 Least Squares   F-statistic:                     1071.\n",
       "Date:                Mon, 10 Jun 2019   Prob (F-statistic):               0.00\n",
       "Time:                        02:07:44   Log-Likelihood:                 10083.\n",
       "No. Observations:                5471   AIC:                        -2.016e+04\n",
       "Df Residuals:                    5469   BIC:                        -2.015e+04\n",
       "Df Model:                           2                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "===============================================================================\n",
       "                  coef    std err          t      P>|t|      [0.025      0.975]\n",
       "-------------------------------------------------------------------------------\n",
       "mean_ivl    -2.492e-05   2.64e-06     -9.429      0.000   -3.01e-05   -1.97e-05\n",
       "mean_factor  1.809e-05    5.5e-07     32.881      0.000     1.7e-05    1.92e-05\n",
       "==============================================================================\n",
       "Omnibus:                      609.617   Durbin-Watson:                   1.884\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              777.624\n",
       "Skew:                           0.902   Prob(JB):                    1.38e-169\n",
       "Kurtosis:                       2.608   Cond. No.                         8.96\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**reps** = work done to remember a card  \n",
    "**interval** = memory length as output of memorization work done  \n",
    "**ease/factor** = indicator of effort to retreive & store memory  \n",
    "**lapses** = result of memory deficit, a common side-effect & indicator of inefficiency of memorization efforts  \n",
    "\n",
    "**lapses/reps ratio** (waste ratio) => the closer to 0, the better (\"low waste\"). the higher this is, the worse : \"high waste\"  \n",
    "**interval/reps ratio** (ROI ratio) = the higher the better (\"low effort\" / \"sticky\"). the lower this is, the worse (\"high effort\", \"slippery\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_006_corr = df_notes.copy()\n",
    "df_notes_006_corr = df_notes_006_corr.drop(list(binary_list + card_list + ['nid','jlpt_lvl_d']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TermLen</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "      <th>mean_ivl</th>\n",
       "      <th>mean_factor</th>\n",
       "      <th>mean_reps</th>\n",
       "      <th>mean_lapses</th>\n",
       "      <th>c_suff_reviewed_x</th>\n",
       "      <th>total_reps</th>\n",
       "      <th>total_lapses</th>\n",
       "      <th>c_suff_reviewed_y</th>\n",
       "      <th>mean_note_waste</th>\n",
       "      <th>mean_note_roi</th>\n",
       "      <th>n_ivl_q</th>\n",
       "      <th>n_factor_q</th>\n",
       "      <th>n_waste_q</th>\n",
       "      <th>n_roi_q</th>\n",
       "      <th>no_waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.0</td>\n",
       "      <td>5471.000000</td>\n",
       "      <td>5471.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.629318</td>\n",
       "      <td>4.195028</td>\n",
       "      <td>1.0</td>\n",
       "      <td>290.752391</td>\n",
       "      <td>1667.835618</td>\n",
       "      <td>16.423780</td>\n",
       "      <td>0.638884</td>\n",
       "      <td>1.0</td>\n",
       "      <td>18.480351</td>\n",
       "      <td>0.747395</td>\n",
       "      <td>1.110400</td>\n",
       "      <td>0.026551</td>\n",
       "      <td>22.462620</td>\n",
       "      <td>1.917200</td>\n",
       "      <td>0.912265</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.849936</td>\n",
       "      <td>0.608847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.178436</td>\n",
       "      <td>1.704865</td>\n",
       "      <td>0.0</td>\n",
       "      <td>203.692803</td>\n",
       "      <td>354.165101</td>\n",
       "      <td>8.436636</td>\n",
       "      <td>1.067492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.527311</td>\n",
       "      <td>1.257439</td>\n",
       "      <td>0.319196</td>\n",
       "      <td>0.036583</td>\n",
       "      <td>17.278964</td>\n",
       "      <td>1.379498</td>\n",
       "      <td>0.792548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.352079</td>\n",
       "      <td>0.488053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>163.000000</td>\n",
       "      <td>1300.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.377976</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>230.000000</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.437500</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>374.500000</td>\n",
       "      <td>1947.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>32.121429</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1749.000000</td>\n",
       "      <td>2650.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>105.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>77.583333</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           TermLen    Syllables  c_suff_reviewed     mean_ivl  mean_factor  \\\n",
       "count  5471.000000  5471.000000           5471.0  5471.000000  5471.000000   \n",
       "mean      2.629318     4.195028              1.0   290.752391  1667.835618   \n",
       "std       1.178436     1.704865              0.0   203.692803   354.165101   \n",
       "min       1.000000     1.000000              1.0     1.000000  1300.000000   \n",
       "25%       2.000000     3.000000              1.0   163.000000  1300.000000   \n",
       "50%       2.000000     4.000000              1.0   230.000000  1600.000000   \n",
       "75%       3.000000     5.000000              1.0   374.500000  1947.000000   \n",
       "max      15.000000    30.000000              1.0  1749.000000  2650.000000   \n",
       "\n",
       "         mean_reps  mean_lapses  c_suff_reviewed_x   total_reps  total_lapses  \\\n",
       "count  5471.000000  5471.000000             5471.0  5471.000000   5471.000000   \n",
       "mean     16.423780     0.638884                1.0    18.480351      0.747395   \n",
       "std       8.436636     1.067492                0.0    11.527311      1.257439   \n",
       "min       7.000000     0.000000                1.0     7.000000      0.000000   \n",
       "25%      11.000000     0.000000                1.0    11.000000      0.000000   \n",
       "50%      14.000000     0.000000                1.0    15.000000      0.000000   \n",
       "75%      19.000000     1.000000                1.0    22.000000      1.000000   \n",
       "max      81.000000    10.000000                1.0   105.000000     13.000000   \n",
       "\n",
       "       c_suff_reviewed_y  mean_note_waste  mean_note_roi      n_ivl_q  \\\n",
       "count        5471.000000      5471.000000    5471.000000  5471.000000   \n",
       "mean            1.110400         0.026551      22.462620     1.917200   \n",
       "std             0.319196         0.036583      17.278964     1.379498   \n",
       "min             1.000000         0.000000       0.019608     0.000000   \n",
       "25%             1.000000         0.000000       9.377976     1.000000   \n",
       "50%             1.000000         0.000000      17.437500     2.000000   \n",
       "75%             1.000000         0.055556      32.121429     3.000000   \n",
       "max             3.000000         0.138462      77.583333     4.000000   \n",
       "\n",
       "        n_factor_q  n_waste_q      n_roi_q     no_waste  \n",
       "count  5471.000000     5471.0  5471.000000  5471.000000  \n",
       "mean      0.912265        0.0     1.849936     0.608847  \n",
       "std       0.792548        0.0     1.352079     0.488053  \n",
       "min       0.000000        0.0     0.000000     0.000000  \n",
       "25%       0.000000        0.0     1.000000     0.000000  \n",
       "50%       1.000000        0.0     2.000000     1.000000  \n",
       "75%       2.000000        0.0     3.000000     1.000000  \n",
       "max       2.000000        0.0     4.000000     1.000000  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_006_corr.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TermLen</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "      <th>mean_ivl</th>\n",
       "      <th>mean_factor</th>\n",
       "      <th>mean_reps</th>\n",
       "      <th>mean_lapses</th>\n",
       "      <th>c_suff_reviewed_x</th>\n",
       "      <th>total_reps</th>\n",
       "      <th>total_lapses</th>\n",
       "      <th>c_suff_reviewed_y</th>\n",
       "      <th>mean_note_waste</th>\n",
       "      <th>mean_note_roi</th>\n",
       "      <th>n_ivl_q</th>\n",
       "      <th>n_factor_q</th>\n",
       "      <th>n_waste_q</th>\n",
       "      <th>n_roi_q</th>\n",
       "      <th>no_waste</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TermLen</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.690789</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.057139</td>\n",
       "      <td>0.064242</td>\n",
       "      <td>-0.111862</td>\n",
       "      <td>-0.101826</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.092871</td>\n",
       "      <td>-0.095772</td>\n",
       "      <td>-0.013661</td>\n",
       "      <td>-0.091540</td>\n",
       "      <td>0.101373</td>\n",
       "      <td>0.079516</td>\n",
       "      <td>0.071068</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.105291</td>\n",
       "      <td>0.093369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Syllables</th>\n",
       "      <td>0.690789</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>0.028122</td>\n",
       "      <td>-0.068392</td>\n",
       "      <td>-0.064827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.058145</td>\n",
       "      <td>-0.064425</td>\n",
       "      <td>-0.016393</td>\n",
       "      <td>-0.055460</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.025309</td>\n",
       "      <td>0.051272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_suff_reviewed</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_ivl</th>\n",
       "      <td>0.057139</td>\n",
       "      <td>-0.012037</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.272481</td>\n",
       "      <td>-0.205835</td>\n",
       "      <td>-0.236627</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.033469</td>\n",
       "      <td>-0.173074</td>\n",
       "      <td>0.249497</td>\n",
       "      <td>-0.232346</td>\n",
       "      <td>0.828531</td>\n",
       "      <td>0.856516</td>\n",
       "      <td>0.286458</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770904</td>\n",
       "      <td>0.182931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_factor</th>\n",
       "      <td>0.064242</td>\n",
       "      <td>0.028122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.272481</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.623813</td>\n",
       "      <td>-0.384894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.507718</td>\n",
       "      <td>-0.355993</td>\n",
       "      <td>-0.038571</td>\n",
       "      <td>-0.328943</td>\n",
       "      <td>0.620882</td>\n",
       "      <td>0.391021</td>\n",
       "      <td>0.935443</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.643384</td>\n",
       "      <td>0.364352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_reps</th>\n",
       "      <td>-0.111862</td>\n",
       "      <td>-0.068392</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.205835</td>\n",
       "      <td>-0.623813</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.863008</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.837337</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.090394</td>\n",
       "      <td>0.657237</td>\n",
       "      <td>-0.536322</td>\n",
       "      <td>-0.350183</td>\n",
       "      <td>-0.626221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.615051</td>\n",
       "      <td>-0.637113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_lapses</th>\n",
       "      <td>-0.101826</td>\n",
       "      <td>-0.064827</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.236627</td>\n",
       "      <td>-0.384894</td>\n",
       "      <td>0.863008</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.737426</td>\n",
       "      <td>0.943051</td>\n",
       "      <td>0.111480</td>\n",
       "      <td>0.879332</td>\n",
       "      <td>-0.453439</td>\n",
       "      <td>-0.361457</td>\n",
       "      <td>-0.396082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.533666</td>\n",
       "      <td>-0.746754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_suff_reviewed_x</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_reps</th>\n",
       "      <td>-0.092871</td>\n",
       "      <td>-0.058145</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.033469</td>\n",
       "      <td>-0.507718</td>\n",
       "      <td>0.837337</td>\n",
       "      <td>0.737426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.854085</td>\n",
       "      <td>0.581956</td>\n",
       "      <td>0.608875</td>\n",
       "      <td>-0.381869</td>\n",
       "      <td>-0.169807</td>\n",
       "      <td>-0.492228</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.442506</td>\n",
       "      <td>-0.642494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_lapses</th>\n",
       "      <td>-0.095772</td>\n",
       "      <td>-0.064425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.173074</td>\n",
       "      <td>-0.355993</td>\n",
       "      <td>0.811224</td>\n",
       "      <td>0.943051</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.854085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.344603</td>\n",
       "      <td>0.849807</td>\n",
       "      <td>-0.411947</td>\n",
       "      <td>-0.290925</td>\n",
       "      <td>-0.358493</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.480264</td>\n",
       "      <td>-0.741624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c_suff_reviewed_y</th>\n",
       "      <td>-0.013661</td>\n",
       "      <td>-0.016393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.249497</td>\n",
       "      <td>-0.038571</td>\n",
       "      <td>0.090394</td>\n",
       "      <td>0.111480</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.581956</td>\n",
       "      <td>0.344603</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.193892</td>\n",
       "      <td>0.093759</td>\n",
       "      <td>0.185174</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.078636</td>\n",
       "      <td>-0.304812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_note_waste</th>\n",
       "      <td>-0.091540</td>\n",
       "      <td>-0.055460</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.232346</td>\n",
       "      <td>-0.328943</td>\n",
       "      <td>0.657237</td>\n",
       "      <td>0.879332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.608875</td>\n",
       "      <td>0.849807</td>\n",
       "      <td>0.193892</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.456766</td>\n",
       "      <td>-0.357375</td>\n",
       "      <td>-0.323317</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.534287</td>\n",
       "      <td>-0.905575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_note_roi</th>\n",
       "      <td>0.101373</td>\n",
       "      <td>0.016678</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.828531</td>\n",
       "      <td>0.620882</td>\n",
       "      <td>-0.536322</td>\n",
       "      <td>-0.453439</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.381869</td>\n",
       "      <td>-0.411947</td>\n",
       "      <td>0.093759</td>\n",
       "      <td>-0.456766</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830416</td>\n",
       "      <td>0.596981</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.931291</td>\n",
       "      <td>0.450806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_ivl_q</th>\n",
       "      <td>0.079516</td>\n",
       "      <td>-0.000595</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.856516</td>\n",
       "      <td>0.391021</td>\n",
       "      <td>-0.350183</td>\n",
       "      <td>-0.361457</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.169807</td>\n",
       "      <td>-0.290925</td>\n",
       "      <td>0.185174</td>\n",
       "      <td>-0.357375</td>\n",
       "      <td>0.830416</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.403357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.853511</td>\n",
       "      <td>0.323073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_factor_q</th>\n",
       "      <td>0.071068</td>\n",
       "      <td>0.032690</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.286458</td>\n",
       "      <td>0.935443</td>\n",
       "      <td>-0.626221</td>\n",
       "      <td>-0.396082</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.492228</td>\n",
       "      <td>-0.358493</td>\n",
       "      <td>-0.000728</td>\n",
       "      <td>-0.323317</td>\n",
       "      <td>0.596981</td>\n",
       "      <td>0.403357</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.632759</td>\n",
       "      <td>0.356951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_waste_q</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n_roi_q</th>\n",
       "      <td>0.105291</td>\n",
       "      <td>0.025309</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770904</td>\n",
       "      <td>0.643384</td>\n",
       "      <td>-0.615051</td>\n",
       "      <td>-0.533666</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.442506</td>\n",
       "      <td>-0.480264</td>\n",
       "      <td>0.078636</td>\n",
       "      <td>-0.534287</td>\n",
       "      <td>0.931291</td>\n",
       "      <td>0.853511</td>\n",
       "      <td>0.632759</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.532434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>no_waste</th>\n",
       "      <td>0.093369</td>\n",
       "      <td>0.051272</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.182931</td>\n",
       "      <td>0.364352</td>\n",
       "      <td>-0.637113</td>\n",
       "      <td>-0.746754</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.642494</td>\n",
       "      <td>-0.741624</td>\n",
       "      <td>-0.304812</td>\n",
       "      <td>-0.905575</td>\n",
       "      <td>0.450806</td>\n",
       "      <td>0.323073</td>\n",
       "      <td>0.356951</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.532434</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    TermLen  Syllables  c_suff_reviewed  mean_ivl  \\\n",
       "TermLen            1.000000   0.690789              NaN  0.057139   \n",
       "Syllables          0.690789   1.000000              NaN -0.012037   \n",
       "c_suff_reviewed         NaN        NaN              NaN       NaN   \n",
       "mean_ivl           0.057139  -0.012037              NaN  1.000000   \n",
       "mean_factor        0.064242   0.028122              NaN  0.272481   \n",
       "mean_reps         -0.111862  -0.068392              NaN -0.205835   \n",
       "mean_lapses       -0.101826  -0.064827              NaN -0.236627   \n",
       "c_suff_reviewed_x       NaN        NaN              NaN       NaN   \n",
       "total_reps        -0.092871  -0.058145              NaN -0.033469   \n",
       "total_lapses      -0.095772  -0.064425              NaN -0.173074   \n",
       "c_suff_reviewed_y -0.013661  -0.016393              NaN  0.249497   \n",
       "mean_note_waste   -0.091540  -0.055460              NaN -0.232346   \n",
       "mean_note_roi      0.101373   0.016678              NaN  0.828531   \n",
       "n_ivl_q            0.079516  -0.000595              NaN  0.856516   \n",
       "n_factor_q         0.071068   0.032690              NaN  0.286458   \n",
       "n_waste_q               NaN        NaN              NaN       NaN   \n",
       "n_roi_q            0.105291   0.025309              NaN  0.770904   \n",
       "no_waste           0.093369   0.051272              NaN  0.182931   \n",
       "\n",
       "                   mean_factor  mean_reps  mean_lapses  c_suff_reviewed_x  \\\n",
       "TermLen               0.064242  -0.111862    -0.101826                NaN   \n",
       "Syllables             0.028122  -0.068392    -0.064827                NaN   \n",
       "c_suff_reviewed            NaN        NaN          NaN                NaN   \n",
       "mean_ivl              0.272481  -0.205835    -0.236627                NaN   \n",
       "mean_factor           1.000000  -0.623813    -0.384894                NaN   \n",
       "mean_reps            -0.623813   1.000000     0.863008                NaN   \n",
       "mean_lapses          -0.384894   0.863008     1.000000                NaN   \n",
       "c_suff_reviewed_x          NaN        NaN          NaN                NaN   \n",
       "total_reps           -0.507718   0.837337     0.737426                NaN   \n",
       "total_lapses         -0.355993   0.811224     0.943051                NaN   \n",
       "c_suff_reviewed_y    -0.038571   0.090394     0.111480                NaN   \n",
       "mean_note_waste      -0.328943   0.657237     0.879332                NaN   \n",
       "mean_note_roi         0.620882  -0.536322    -0.453439                NaN   \n",
       "n_ivl_q               0.391021  -0.350183    -0.361457                NaN   \n",
       "n_factor_q            0.935443  -0.626221    -0.396082                NaN   \n",
       "n_waste_q                  NaN        NaN          NaN                NaN   \n",
       "n_roi_q               0.643384  -0.615051    -0.533666                NaN   \n",
       "no_waste              0.364352  -0.637113    -0.746754                NaN   \n",
       "\n",
       "                   total_reps  total_lapses  c_suff_reviewed_y  \\\n",
       "TermLen             -0.092871     -0.095772          -0.013661   \n",
       "Syllables           -0.058145     -0.064425          -0.016393   \n",
       "c_suff_reviewed           NaN           NaN                NaN   \n",
       "mean_ivl            -0.033469     -0.173074           0.249497   \n",
       "mean_factor         -0.507718     -0.355993          -0.038571   \n",
       "mean_reps            0.837337      0.811224           0.090394   \n",
       "mean_lapses          0.737426      0.943051           0.111480   \n",
       "c_suff_reviewed_x         NaN           NaN                NaN   \n",
       "total_reps           1.000000      0.854085           0.581956   \n",
       "total_lapses         0.854085      1.000000           0.344603   \n",
       "c_suff_reviewed_y    0.581956      0.344603           1.000000   \n",
       "mean_note_waste      0.608875      0.849807           0.193892   \n",
       "mean_note_roi       -0.381869     -0.411947           0.093759   \n",
       "n_ivl_q             -0.169807     -0.290925           0.185174   \n",
       "n_factor_q          -0.492228     -0.358493          -0.000728   \n",
       "n_waste_q                 NaN           NaN                NaN   \n",
       "n_roi_q             -0.442506     -0.480264           0.078636   \n",
       "no_waste            -0.642494     -0.741624          -0.304812   \n",
       "\n",
       "                   mean_note_waste  mean_note_roi   n_ivl_q  n_factor_q  \\\n",
       "TermLen                  -0.091540       0.101373  0.079516    0.071068   \n",
       "Syllables                -0.055460       0.016678 -0.000595    0.032690   \n",
       "c_suff_reviewed                NaN            NaN       NaN         NaN   \n",
       "mean_ivl                 -0.232346       0.828531  0.856516    0.286458   \n",
       "mean_factor              -0.328943       0.620882  0.391021    0.935443   \n",
       "mean_reps                 0.657237      -0.536322 -0.350183   -0.626221   \n",
       "mean_lapses               0.879332      -0.453439 -0.361457   -0.396082   \n",
       "c_suff_reviewed_x              NaN            NaN       NaN         NaN   \n",
       "total_reps                0.608875      -0.381869 -0.169807   -0.492228   \n",
       "total_lapses              0.849807      -0.411947 -0.290925   -0.358493   \n",
       "c_suff_reviewed_y         0.193892       0.093759  0.185174   -0.000728   \n",
       "mean_note_waste           1.000000      -0.456766 -0.357375   -0.323317   \n",
       "mean_note_roi            -0.456766       1.000000  0.830416    0.596981   \n",
       "n_ivl_q                  -0.357375       0.830416  1.000000    0.403357   \n",
       "n_factor_q               -0.323317       0.596981  0.403357    1.000000   \n",
       "n_waste_q                      NaN            NaN       NaN         NaN   \n",
       "n_roi_q                  -0.534287       0.931291  0.853511    0.632759   \n",
       "no_waste                 -0.905575       0.450806  0.323073    0.356951   \n",
       "\n",
       "                   n_waste_q   n_roi_q  no_waste  \n",
       "TermLen                  NaN  0.105291  0.093369  \n",
       "Syllables                NaN  0.025309  0.051272  \n",
       "c_suff_reviewed          NaN       NaN       NaN  \n",
       "mean_ivl                 NaN  0.770904  0.182931  \n",
       "mean_factor              NaN  0.643384  0.364352  \n",
       "mean_reps                NaN -0.615051 -0.637113  \n",
       "mean_lapses              NaN -0.533666 -0.746754  \n",
       "c_suff_reviewed_x        NaN       NaN       NaN  \n",
       "total_reps               NaN -0.442506 -0.642494  \n",
       "total_lapses             NaN -0.480264 -0.741624  \n",
       "c_suff_reviewed_y        NaN  0.078636 -0.304812  \n",
       "mean_note_waste          NaN -0.534287 -0.905575  \n",
       "mean_note_roi            NaN  0.931291  0.450806  \n",
       "n_ivl_q                  NaN  0.853511  0.323073  \n",
       "n_factor_q               NaN  0.632759  0.356951  \n",
       "n_waste_q                NaN       NaN       NaN  \n",
       "n_roi_q                  NaN  1.000000  0.532434  \n",
       "no_waste                 NaN  0.532434  1.000000  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_006_corr.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>nid</th>\n",
       "      <th>tags</th>\n",
       "      <th>Term</th>\n",
       "      <th>Yomi1</th>\n",
       "      <th>NoteCreated</th>\n",
       "      <th>LastModified</th>\n",
       "      <th>commonword</th>\n",
       "      <th>clothing</th>\n",
       "      <th>animal</th>\n",
       "      <th>body</th>\n",
       "      <th>food</th>\n",
       "      <th>place</th>\n",
       "      <th>textbook</th>\n",
       "      <th>college</th>\n",
       "      <th>fromdict</th>\n",
       "      <th>fromexam</th>\n",
       "      <th>n1</th>\n",
       "      <th>n2</th>\n",
       "      <th>n3</th>\n",
       "      <th>n4</th>\n",
       "      <th>n5</th>\n",
       "      <th>katakana</th>\n",
       "      <th>hiragana</th>\n",
       "      <th>kanji</th>\n",
       "      <th>adv</th>\n",
       "      <th>adj</th>\n",
       "      <th>noun</th>\n",
       "      <th>verb</th>\n",
       "      <th>nonconvo</th>\n",
       "      <th>convo</th>\n",
       "      <th>metalite</th>\n",
       "      <th>hasSimilarSound</th>\n",
       "      <th>hasSameSound</th>\n",
       "      <th>hasVisual</th>\n",
       "      <th>hasAudio</th>\n",
       "      <th>hasMultiMeaning</th>\n",
       "      <th>hasMultiReading</th>\n",
       "      <th>hasSimilarMeaning</th>\n",
       "      <th>hasAltForm</th>\n",
       "      <th>hasRichExamples</th>\n",
       "      <th>TermLen</th>\n",
       "      <th>Syllables</th>\n",
       "      <th>TermLenGroup</th>\n",
       "      <th>SyllablesGroup</th>\n",
       "      <th>jlpt_lvl_d</th>\n",
       "      <th>script</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "      <th>mean_ivl</th>\n",
       "      <th>mean_factor</th>\n",
       "      <th>mean_reps</th>\n",
       "      <th>mean_lapses</th>\n",
       "      <th>c_suff_reviewed_x</th>\n",
       "      <th>total_reps</th>\n",
       "      <th>total_lapses</th>\n",
       "      <th>hasListenCard</th>\n",
       "      <th>hasPictureCard</th>\n",
       "      <th>hasReadCard</th>\n",
       "      <th>hasTranslateCard</th>\n",
       "      <th>c_suff_reviewed_y</th>\n",
       "      <th>mean_note_waste</th>\n",
       "      <th>mean_note_roi</th>\n",
       "      <th>n_ivl_q</th>\n",
       "      <th>n_factor_q</th>\n",
       "      <th>n_waste_q</th>\n",
       "      <th>n_roi_q</th>\n",
       "      <th>no_waste</th>\n",
       "      <th>analysis_cat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [nid, tags, Term, Yomi1, NoteCreated, LastModified, commonword, clothing, animal, body, food, place, textbook, college, fromdict, fromexam, n1, n2, n3, n4, n5, katakana, hiragana, kanji, adv, adj, noun, verb, nonconvo, convo, metalite, hasSimilarSound, hasSameSound, hasVisual, hasAudio, hasMultiMeaning, hasMultiReading, hasSimilarMeaning, hasAltForm, hasRichExamples, TermLen, Syllables, TermLenGroup, SyllablesGroup, jlpt_lvl_d, script, c_suff_reviewed, mean_ivl, mean_factor, mean_reps, mean_lapses, c_suff_reviewed_x, total_reps, total_lapses, hasListenCard, hasPictureCard, hasReadCard, hasTranslateCard, c_suff_reviewed_y, mean_note_waste, mean_note_roi, n_ivl_q, n_factor_q, n_waste_q, n_roi_q, no_waste, analysis_cat]\n",
       "Index: []"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rows_by_value_in_col(df_notes, 1523892839900, 'nid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cid</th>\n",
       "      <th>nid</th>\n",
       "      <th>ivl</th>\n",
       "      <th>factor</th>\n",
       "      <th>reps</th>\n",
       "      <th>lapses</th>\n",
       "      <th>CardCreated</th>\n",
       "      <th>DueDate</th>\n",
       "      <th>c_ivl_q</th>\n",
       "      <th>c_factor_q</th>\n",
       "      <th>CardType_listen</th>\n",
       "      <th>CardType_look</th>\n",
       "      <th>CardType_read</th>\n",
       "      <th>CardType_recall</th>\n",
       "      <th>cardtype</th>\n",
       "      <th>waste</th>\n",
       "      <th>roi</th>\n",
       "      <th>c_suff_reviewed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [cid, nid, ivl, factor, reps, lapses, CardCreated, DueDate, c_ivl_q, c_factor_q, CardType_listen, CardType_look, CardType_read, CardType_recall, cardtype, waste, roi, c_suff_reviewed]\n",
       "Index: []"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rows_by_value_in_col(df_cards, 1523892839900, 'nid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_notes_n_corr = df_notes.copy()\n",
    "df_notes_n_corr = df_notes_n_corr.drop(list(binary_list + card_list + \n",
    "    ['nid','tags','Term','Yomi1','jlpt_lvl_d','total_reps','total_lapses',\n",
    "     'n_ivl_q', 'n_factor_q', 'n_waste_q', 'n_roi_q', 'no_waste']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NoteCreated', 'LastModified', 'TermLen', 'Syllables',\n",
       "       'TermLenGroup', 'SyllablesGroup', 'script', 'c_suff_reviewed',\n",
       "       'mean_ivl', 'mean_factor', 'mean_reps', 'mean_lapses',\n",
       "       'c_suff_reviewed_x', 'c_suff_reviewed_y', 'mean_note_waste',\n",
       "       'mean_note_roi', 'analysis_cat'], dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_notes_n_corr.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic correlogram\n",
    "if graphs_on:\n",
    "    g = sns.pairplot(df_notes_n_corr)\n",
    "    g.fig.suptitle(\"Note Correlogram\", y=1.05, fontsize=24)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ivl', 'factor', 'reps', 'lapses', 'CardCreated', 'cardtype',\n",
       "       'waste', 'roi', 'c_suff_reviewed'], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cards_n_corr = df_cards.copy()\n",
    "\n",
    "df_cards_n_corr = df_cards_n_corr.drop(['cid','nid','CardType_listen','DueDate',\n",
    "    'CardType_look','CardType_read','CardType_recall','c_ivl_q', 'c_factor_q'], axis=1)\n",
    "\n",
    "df_cards_n_corr.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic correlogram\n",
    "if graphs_on:\n",
    "    g = sns.pairplot(df_cards_n_corr)\n",
    "    g.fig.suptitle(\"Card Correlogram\", y=1.05, fontsize=24)\n",
    "    # save image out\n",
    "    save_fig_a(image_cnt)\n",
    "    image_cnt = image_cnt + 1\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intitial Analysis\n",
    "\n",
    "There appears to be a linear relationship between lapses & reps. It appears that lapses incur a cost of increasing reps. However, this info isn't directly actionable - one cannot simply just 'not forget'. The primary focus is what can be done to minimize lapses while not sacrificing efficiency (long intervals for few reps, and lapses as close as possible to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topical Analysis\n",
    "\n",
    "After doing some basic assessments of the data, we can dig a bit deeper:\n",
    "- Is there a correlation between words having multiple readings (\"yomi\") and their forget rate\\*?\n",
    "- Is there a correlation between words having same/similar sounding words and their forget rate\\*?\n",
    "- What might the effect of word length be on memorability? \\*\\*, \\*\\*\\*\n",
    "\n",
    "> \\* Forget rate can be understand as a multitude of things, such as the ratio between lapses & reps, as well as the raw lapse count, the average interval, and other numbers/ratios to be determined. I will attempt to clarify this in the process.  \n",
    "\\*\\* Memorability being loosely correlated with forget rate, where memorability could be understood as a word/term's intrinsic \"stickiness\" in the brain, as opposed to an individual or collective's capacity to keep words/terms in their head. Sources pending.  \n",
    "\\*\\*\\* A huge caveat here being that, this dataset has a sample size of 1 (for both student and language), so all observations, interpretations, and understandings must be taken with more than a few grains of salt (and tested further with larger sample sizes, of at least 200 students, and 5 or more languages)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Analysis\n",
    "\n",
    "For a deeper understanding of what it means to aquire new terminology, the researcher believes it best to conduct analysis on term acquisition by merging multiple vectors (individual cards) of a single term into single entries, where dummy values for each vector (such as review count, lapse count, etc.) are encoded per entry. This would enable inspection and correlation analysis of:\n",
    "- total reviews per term\n",
    "- average ratio of reviews per term per vector (look vs hear vs recall vs read)\n",
    "- where lapses are most likely to occur (per word, per vector, etc.)\n",
    "- how word length, presence of kanji, katakana, hirgana, or combination thereof, may affect the above counts & ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Further Information\n",
    "\n",
    "The Spaced Repetition Software (\\\"SRS\\\") used for the study of Japanese by student \\\"A\\\" is an open souce program called Anki. The algorithm used by it to \\\"graduate\\\" (also refered to as \\\"maturing\\\") study items (called cards) so that subsequent reviews/practices will be spaced into the future is referred to as SM-2. [Please click here for more information on the SM-2 algorithm used in Anki.](\"https://apps.ankiweb.net/docs/manual.html#what-algorithm\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#show correlation of stats via heatmap\n",
    "df_worked = df_binary2.copy() # 'ivl','factor','lapses'\n",
    "graph_drop_cols_1 = [\n",
    "    'nid','commonword','clothing','animal','body','food','textbook','college','place',\n",
    "    'fromdict','fromexam','onechar','n1','n2','n3','n4','n5','hasVisual',\n",
    "    'hasAudio','hasSimilar','hasAltForm','TermLen','Syllables','ivl_q']\n",
    "df_worked = df_worked.drop(graph_drop_cols_1,axis=1)\n",
    "corr = df_worked.corr()\n",
    "# https://stackoverflow.com/questions/38913965/make-the-size-of-a-heatmap-bigger-with-seaborn\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "sns.heatmap(corr, vmin=-1, annot=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
