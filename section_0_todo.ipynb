{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Todo\n",
    "\n",
    "links:  \n",
    "https://github.com/avidrucker/vocab-analysis  \n",
    "https://docs.google.com/presentation/d/1UntQmGL2uhH9POCQzyVEh2j7qviHmrZjjyHJttlvVWU/edit?usp=sharing  \n",
    "https://github.com/ankidroid/Anki-Android/wiki/Database-Structure )\n",
    "\n",
    "general:\n",
    "- ~~upload this document to the dedicated github project repository~~\n",
    "- consolidate all paper todos, critique advices, etc. to this document **in_progress**\n",
    "\n",
    "section 1 (questions) :\n",
    "- assert that each question is smart \n",
    "\n",
    "section 2 (data prep) :\n",
    "- mark fields as binary, continuous, or discrete **in_progress**\n",
    "- add average ease (mean_factor) for each card type (using the CARDS! dataframe)\n",
    "- combine revlogs w/ combo dataframe to tabulate review time, review count, lapse count, for both per card and per note\n",
    "- save out csv after each section, label \"section_#_output\", import into the following section\n",
    "    - ~~section 2 notes~~\n",
    "    - ~~section 2 cards~~\n",
    "    - section 2 revlogs\n",
    "    - ~~section 2 note_card_combo~~\n",
    "        - binary only\n",
    "        - non-binary only\n",
    "        - ~~full combo~~\n",
    "- suffix \"final\" to card, note, and revlog dataframes to clearly denote analysis readiness\n",
    "    - ~~cards~~\n",
    "    - ~~notes~~\n",
    "    - revlogs\n",
    "- Convert (& preserve) RevLog ID to reviewed date\n",
    "- Add list of review dates to each card\n",
    "\n",
    "section 3 (analysis) :\n",
    "- read over modules 6, 7, 8 and 9 for methods of analysis, choose 2-3 that look esp. promising to use **in_progress**\n",
    "- explore 3-5 features for feature analysis\n",
    "- combine mean_notes, sum_notes & final_notes, then inspect post office:\n",
    "```\n",
    "selX = inspect_note_by_term(df_notes,'郵便')\n",
    "selX\n",
    "```\n",
    "- also inspect 'kurenai' crimison red: `inspect_note_by_id(df_notes_002_better_tags,note_id_1)`\n",
    "\n",
    "\n",
    "section 4 (graphing) :\n",
    "- inspect most common tags: make this into a word cloud\n",
    "- re-render all the graphs after you have exported the newest, clean data\n",
    "- create new graphs (use broad survey & quick correlation graphs from recent exercises/modules)\n",
    "- assert that, each has a title, each has a human readible (ie. easily understandable) axis label for each axis that exists, that each category has a human readable category, etc..\n",
    "- throw dataframe csv's into Tableau to make graph jam\n",
    "- export graphs from Anki\n",
    "\n",
    "\n",
    "section 5 (presentation) :\n",
    "- update presentation with new graphs\n",
    "\n",
    "\n",
    "done:\n",
    "- ~~Increase dataset to include all Japanese vocabulary from your Anki~~\n",
    "- ~~add todos from email reminder (see email inbox)~~\n",
    "- ~~remove words with 0 reps from primary graphing, as they dilute study consequence correlations/readings/analyses~~\n",
    "- ~~create dedicated github project repository~~ https://github.com/avidrucker/vocab-analysis\n",
    "- ~~link to presentation~~: https://docs.google.com/presentation/d/1UntQmGL2uhH9POCQzyVEh2j7qviHmrZjjyHJttlvVWU/edit?usp=sharing\n",
    "- ~~fix ordering (1) tag removal, renaming & confirmation (tag frequency), then (2) dataframe merges, then (3) addition of binary columns~~\n",
    "- ~~mark cells for export to sections 3, 4~~\n",
    "- ~~export section 3 & section 4 cells to next documents~~\n",
    "- ~~re-export Anki collection from PC into project, then unzip & rerun with this notebook~~\n",
    "- ~~break up notebook into 5 sections/partitions as per \"DataScienceProcessTips.pdf\"~~\n",
    "- ~~import in Section 1 from personal laptop where question has been identified~~\n",
    "- ~~import in review (revlog) data (see model info here:~~ https://github.com/ankidroid/Anki-Android/wiki/Database-Structure )\n",
    "- ~~export all utility functions to be used in other notebooks~~\n",
    "- ~~add assertion to confirm that sel3 is empty (row size of 0)~~\n",
    "- ~~apply date conversions before dataframe merge, not after~~\n",
    "- ~~Remove sentences, questions & phrases to prevent skewing of data & to hone in on vocabulary trends first. The above can be added back, or analyzed separately, after. (remove sentences, questions & phrases, idioms too)~~\n",
    "- ~~clearly name card, note, and revlog dataframes as numbered steps~~\n",
    "    - ~~cards~~\n",
    "    - ~~notes~~\n",
    "    - ~~revlogs~~\n",
    "- ~~add average ease (mean_factor) for each note~~\n",
    "- ~~assign (in section 2) JLPT \"N\" levels to each word w/ a JLPT \"N\" tag~~\n",
    "- ~~remove rare words~~\n",
    "- ~~remove phrases, expressions, questions & sentences~~\n",
    "- ~~Before making graphs & charts, prepare a dataframe with only numerical data (binary, non-binary, and (?) both)~~\n",
    "    - ~~ie. remove: 'Yomi1', 'Translation','Translation2', 'Translation3', 'AlternateForms', 'PartOfSpeech', 'Sound', 'Sound2', 'Sound3', 'Examples', 'ExamplesAudio', 'AtoQ','AtoQaudio', 'AtoQkana', 'AtoQtranslation', 'QandApicture','answerPicture', 'Meaning1', 'SimilarWords', 'RelatedWords','Breakdown1', 'Comparison', 'Usage', 'Prompt1', 'Prompt2','KakuMCD', 'IuMCD', 'ExtraMemo', 'Yomi2', 'Meaning2', 'Breakdown2','Picture1', 'Picture2', 'Picture3', 'Picture4', 'HinshiMarker','Hint', 'Term2', 'ArabicNumeral', 'CounterKanji', 'Mnemonic','SameSoundWords', 'Yomi3', 'gChap', 'gBook', 'semester', 'gNumber','Transliteration', 'SoloLookCards', 'TagOverflow', 'blank1','blank2',~~\n",
    "- ~~add binary flag for: katakana, hiragana, food, verb, convo, textbook, fromdict, len1, noun~~\n",
    "\n",
    "for next time:\n",
    "- we want to understand the words conceptually: abstract vs concrete, verbs vs nouns vs adjectives\n",
    "- conduct manual data entry to apply \"concrete\" or \"abstract\" to each note\n",
    "- inspect mod as a marker for 'freshness'\n",
    "- notes: logistic regression: classification/categorization\n",
    "- add 'commonword' tag to tags from 'PartOfSpeech' column\n",
    "- Increase dataset to include all of J\\'s, C\\'s, W's, and E\\'s study data\n",
    "- use TPR list to apply \"demoable\" metatag\n",
    "- fix time zone errors (for times you were in Japan, flip the time zone)\n",
    "- add binary flag for: kana, waseigo, hasrobo, \n",
    "\n",
    "\n",
    "Further Ideas:\n",
    "- ~~generate power as work (reps) over time~~\n",
    "- ~~use lapses to calculate efficiency per word~~\n",
    "- generate stress as ratio of lapses to reps, compare wtih ease\n",
    "\n",
    "\n",
    "questions worth asking:\n",
    "- which are the easiest words to learn first?\n",
    "- which words, when learned first, will set the learner up for better results to achieve their goals (depending on which goals they have chosen)?\n",
    "- which words were learned in context, and which without, and what were the memorability/retention consequences?\n",
    "- how might learning Kanji meanings affect vocabulary retention?\n",
    "\n",
    "ask some questions:\n",
    "- Which learning vectors offer the best ROI? (long memory intervals for few reviews)\n",
    "- What is the cost of failure (forgetting & resetting memories)?\n",
    "- As stress (by-product of effort/difficulty/failure) & time/energy (resource) can be clearly metricized & visualized, how can learners best direct their learning efforts? (little effort over long time vs or big effort over short, or a goldilocks ratio per the person & their current day-to-day, minute-to-minute state of being)\n",
    "- What language/word features affect their memorability (memory durability, storability, & retrievability)?\n",
    "\n",
    "steps:\n",
    "1. import some data\n",
    "2. inspect the data\n",
    "3. determine what to do with the data\n",
    "4. do that\n",
    "5. make data observations & re-evaluate questions\n",
    "\t- what answers we we have?\n",
    "\t- what further questions are generated?\n",
    "\t- which questions remain unanswered?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
